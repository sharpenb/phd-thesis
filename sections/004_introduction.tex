\chapter{Introduction}
\label{chap:introduction}

\epigraph{As far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.}{\textit{Albert Einstein}}

\epigraph{Madness is the consequence not of uncertainty but of certainty.}{-- \textit{Friedrich W. Nietzsche}}

Artificial Intelligence (AI) consists in the intelligence demonstrated by machines in contrast with intelligence demonstrated by humans or animals. 
Hence, AI models are usually computing systems able to perform tasks which would normally require human intelligence, such as visual perception, speech recognition, decision-making, translation between languages, or even art generation.
AI includes both Machine Learning (ML) which consists in the study of computer programs which automatically learn from experience \citep{Mitchell97}, and Deep Learning (DL) which is a subfield of ML and can be defined as models which hierarchically learn complex representations based on simpler ones \citep{GoodBengCour16}.
AI is a very important technology with high potential \emph{economic} and \emph{ethical} impacts. 

Economically, AI has a high momentum. Indeed, 48\% of companies claim that they use AI \cite{ai-market-oreilly} and 83\% companies have AI is their priority \cite{ai-market-forbes}. Overall, the market value of AI is \$136 billion in 2022 \cite{ai-market} and had a Compound Annual Growth Rate (CAGR) of 38.5\% between 2022-2030 \cite{ai-market}, thus indicating a very fast growth.
Hence, AI finds many applications in industry and science. 
Industry applications include e.g. agriculture (e.g. Cropin, Iron Ox, FarmWise), manufacturing (e.g. Exotec, Bright Machines, Fieldbox), automotive industry (e.g. Tesla, Waymo), medicine (e.g. Exscientia, Valence Discovery, Radium), construction (Procore Technologies, OpenSpace), finance (e.g. Affirm, Kreditech, Kayrros), education (e.g. Duolingo, Age of Learning), or even AI for art (e.g. Bytedance, OpenAI).
Scientific applications include e.g. physics (e.g. \cite{gao2022abinitio}), chemistry (e.g. \cite{huang2021abinitio}), biology (e.g. \cite{hetzel2022predicting}), or even mathematics (e.g. \cite{cobbe2021math}).
In all these applications, AI have shown to be applicable with various data types (e.g. images, tabular, text, graphs) and at various scales from very small (e.g. quantum systems or molecules) to very large systems (e.g. social networks or climate).

Ethically, AI systems raises multiple major concerns \cite{bartneck2021ethic}.
It does not guarantee a safe behavior and can create accidents \cite{amodei2019problems}.
It can generate fake data, thus potentially to fake news \cite{nguyen2022deepfakes}.
It does not provide clear explanations for its predictions or provide unreliable explanations \cite{overview-interpretable-ml,interpretable-ml, krishnan2020against}.
It can be racist and discriminate minorities, e.g., due to biased data or manipulation of learning by users \cite{mehrabi2019fairness}.
It can partially replace human jobs by stealing human creations which leaves the question of intellectual property unclear \cite{moerland2022intellectual, gervais2020intellectual}. The replaced jobs include activities like art which can also be enjoyable for humans.
It might reduce individuals’ control over their lives and diminish individuals’ cognitive, social and survival skills as they become dependent on AI \cite{anderson2018concerns}.
It might lead to persistent surveillance and violate data privacy \cite{bartneck2021privacy}.
Finally, it might have goals which are not aligned with human goals. This turns out to be particularly problematic when AI systems achieve super-intelligence \cite{bostrom2014superintelligence} which is realistic in many tasks where AI systems are already better than humans \cite{firestone2020performance}.

Hence, the fast AI economic growth and the multiple AI ethical concerns urge the development of reliable AI models which is the main subject of this thesis.

\section{Why do we need uncertainty estimation?}

\emph{Uncertainty estimation} (a.k.a. \emph{uncertainty quantification}) consists in evaluating the confidence of models in their predictions. This task is crucial for both \emph{practical} and \emph{theoretical} reasons summarized in \cref{fig:uncertainty_reasons}.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[t]{1. \columnwidth}
        \centering
        $\vcenter{\hbox{\includegraphics[width=0.9 \textwidth]{resources/uncertainty_reasons.pdf}}}$
    \end{subfigure}%
    \caption{Overview of the \emph{practical} and \emph{theoretical} reasons for uncertainty estimation.}
    \label{fig:uncertainty_reasons}
	% \vspace{-.5cm}
\end{figure}

\paragraph*{Practical reasons.} The Dunning-Kruger effect \cite{dunning-kruger} describes a psychological cognitive bias in which people lacking knowledge in a particular domain overestimate their abilities. Interestingly, a similar phenomenon also applies to machine learning models. Traditional neural networks show overconfident predictions, in particular on data that are different from the data utilized during training \cite{overconfident-relu}. The illusion of knowledge of machine learning models highly impacts the reliability of such  models  in  safety-critical  domains.
First, it affects the \emph{trust} in ML model predictions. Ideally, we expect ML models to be confident when predicting correctly and uncertain when predicting wrongly. 
Second, it affects the \emph{safety} of ML predictions in unfamiliar situations. Ideally, we expect ML models to flag predictions on unknown domains corresponding to anomaly detection.
Third, it affects the \emph{maintenance} of ML models. Ideally, we expect ML models to become more uncertain when the testing data has drifted away from the training data, thus indicating the need of retraining the model. This is particularly important in application where ML models need to explore and learn all life-long.
And finally, it affects the \emph{fairness} of ML models. Ideally, we expect ML models to provide calibrated predictions on all input regions including underrepresented data.

\paragraph*{Theoretical reasons.} ML models aim at precisely representing the flow of information in the world which is inherently uncertain.
First, the world is \emph{non-deterministic at a large scale}. Indeed, uncertainty emerges when small individual events contribute to macro phenomena like GDP growth, micro phenomena like the growth rate of firms, and non-economic events like war and climate change \cite{macro-micro-uncertainty}.
Second, the world is \emph{non-deterministic at a small scale}. A prominent evidence of this is the uncertainty principle in (quantum) physics. Indeed, the uncertainty principle implies that it is in general not possible to predict the value of a particle quantity (like position and speed) with arbitrary certainty, even if all initial conditions are specified \cite{hilgevoord2016heisenberg}.
Eventually, the world is also only \emph{partially observable}. On one hand, any agent (e.g. human or robot) has to deal with incomplete information on the environment in which it evolves \cite{kaelbling1998pomdp}. Indeed, its available information is limited by its internal sensors which cannot capture any signal type at any resolution. On the other hand, the accessible information is restricted to the observable world which is, at the extreme, limited by the speed of light \cite{ord2021universe}, thus making the world inherently uncertain for any agent (even if it is equipped with perfect sensors).

\paragraph*{} All these reasons underline the need of accurate uncertainty estimation methods in ML. 
Specifically, a reliable ML model should provide high-quality estimates of aleatoric and epistemic uncertainty \citep{uncertainty-deep-learning}.
The \emph{aleatoric uncertainty} allows a model to account for the irreducible data uncertainty (e.g. the inherent sensor noise, or the inherent environment stochasticity).
The \emph{epistemic uncertainty} allows a model to account for its lack of knowledge about unseen data regions (e.g. testing data differs significantly from training data).
Aleatoric and epistemic uncertainty levels can eventually be combined into an overall \emph{predictive uncertainty} \citep{uncertainty-deep-learning}. Hence, this thesis studies the usage of different types of uncertainty for ML methods.


\section{Why do we need independent and non-independent data?}

In this thesis, we consider ML models which process input data to accurately predict output targets. 
More specifically, we expect ML models to be able to process \emph{any type of input data} (e.g. tabular, images, graph or time series) to predict \emph{any type of output data} (e.g. classes, continuous values, counts).
To this end, \emph{independence} and \emph{non-independence} are key assumptions to create \emph{practical} and \emph{accurate} models which precisely describe the real-world. 

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[t]{0.245 \columnwidth}
        \centering
        $\vcenter{\hbox{\includegraphics[width=0.9 \textwidth]{resources/tabular-cropped.pdf}}}$
         \caption{Tabular data}
         \label{fig:tabular_data}
    \end{subfigure}%
    \begin{subfigure}[t]{0.245\columnwidth}
        \centering
        $\vcenter{\hbox{\includegraphics[width=0.9 \textwidth]{resources/image-cropped.pdf}}}$
        \caption{Image data}
        \label{fig:image_data}
    \end{subfigure}%
    \begin{subfigure}[t]{0.245 \columnwidth}
        \centering
        $\vcenter{\hbox{\includegraphics[width=0.9 \textwidth]{resources/graph-cropped.pdf}}}$
         \caption{Graph data}
         \label{fig:graph_data}
    \end{subfigure}%
    \begin{subfigure}[t]{0.245\columnwidth}
        \centering
        $\vcenter{\hbox{\includegraphics[width=0.9 \textwidth]{resources/sequential-cropped.pdf}}}$
        \caption{Sequential data}
        \label{fig:sequential_data}
    \end{subfigure}%
    \caption{Overview of different data types covering independent $\x_i$ inputs like tabular and image data, and dependent $\x_i$ inputs like graph node and sequential time events.}
	% \vspace{-.5cm}
\end{figure}

\paragraph*{Independent Data.} The independence assumption means that different data samples are not connected in any way. In other words, the knowledge of one data sample does not bring any information on another data sample.
The independence assumption is particularly useful when representing tabular data \cite{raschka2022chronology} (e.g. a group of unrelated persons for a medicine trial, defects on multiple different devices) or image data \cite{lu2020survey, litjens2017survey} (e.g. disease detection on medical images of different patients, object detection in different self-driving cars or robots). Visualizations of such data are represented in \cref{fig:tabular_data} and \cref{fig:image_data}.
In this case, representing the interaction between data samples does not bring much information to perform the predictions.
Hence, the key advantage of the independence assumption is that it allows mathematical factorization for practical modelling simplifications \citep{bishop} without important information loss.

\paragraph*{Non-Independent Data.} The non-independence assumption means that different data samples might be connected in some way. In other words, observing a data sample gives some information on the value of another data sample.
The non-independence assumption is particularly useful when representing graph data \cite{GNNBook2022} (e.g. social networks, citation networks) or sequential data \cite{shchur2021review, Dietterich2002Machine} (e.g. financial time series, interaction history of a user). Visualizations of such data are represented in \cref{fig:graph_data} and \cref{fig:sequential_data}.
In this case, neighboring nodes of a graph are expected to share important information and past events are expected to give important information on future events.
Hence, the key advantage of the non-independence assumption is that it retains the information contained in graph and sequential interactions to model the world more precisely.

\paragraph*{} Beyond the \emph{(non-)independence assumption}, another common assumption in ML is that data are \emph{identically distributed}. 
This assumption assumes that all input data comes from the same data distribution which also has strong limitations related to the reasons motivating the usage of uncertainty estimates for ML predictions.
While the training data are assumed to come from the same data distribution, the testing data might come from different distributions and suffer from \emph{Distribution Shifts} \cite{rabanser2019shift, dataset-shift}. 
Indeed, the testing data might come from the \emph{In-Distribution (ID)} similar to the training data, or a \emph{Out-Of-Distribution (OOD)} which could be any distribution different from the training distribution \cite{ood-detection-survey, shen2021ood}.
This scenario frequently happens in \emph{safety} and \emph{maintenance} use-cases where the data distribution observed by the model continuously drifts at testing time. 

Hence, this thesis acknowledges the limitation of the standard \emph{independent and identically distributed (i.i.d.)} assumptions by studying the application of uncertainty estimation to independent and non-independent data.

\section{Contributions and outline}

This thesis studies the application of uncertainty estimation for independent and non-independent data via three main components:
\begin{itemize}
    \item Explicit \emph{desiderata} capturing the desired behavior of uncertainty estimation.
    \item Accurate and efficient \emph{models} for uncertainty estimation with low practical overhead.
    \item Practical \emph{experimental setup} evaluating uncertainty estimation in real-world applications including worst-case scenarios.
\end{itemize} 

In \cref{chap:background}, we start by establishing the background knowledge about uncertainty estimation.

In \cref{part:independent_data}, we present a study of uncertainty estimation for \emph{independent data}: 
In \cref{chap:classification}, we construct a new Bayesian model for uncertainty estimation for classification called \PostNet{} (\PostNetacro{}). \PostNetacro{} requires a \emph{single-forward pass}, does need \emph{no OOD data during training}, and adapt to \emph{many core architectures}.
In \cref{chap:regression}, we construct a new Bayesian model for uncertainty estimation for regression called \NatPN{} (\NatPNacro{}). Beyond regression, \NatPNacro{} applies to a \emph{large variety of supervised tasks} (incl. classification and count prediction) with \emph{very low computational overhead}.
In \cref{chap:robustness}, we present the first study of the robustness of uncertainty models in the worst-case scenario of adversarial attacks. We show that uncertainty estimates of an important family of single-forward pass models are \emph{not robust} for many important real-world tasks (incl. correct/wrong prediction detection, adversarial examples detection, and ID/OOD detection). Further, we explore first approaches methods to improve uncertainty robustness by using \emph{adversarial training} and \emph{randomized smoothing}. 

In \cref{part:non_independent_data}, we present a study of uncertainty estimation for \emph{non-independent data}:
In \cref{chap:graph_data}, we present the first framework for uncertainty estimation for node classification on graph data. This framework proposes explicit desiderata, a new Bayesian model, and an exhaustive evaluation setup which covers aleatoric and epistemic uncertainty estimation \emph{without} and \emph{with network effects}.
In \cref{chap:sequential_data}, we present new models for uncertainty estimation in sequential data with asynchronous time events. The two proposed models are able of accurate \emph{event types} and \emph{event time} predictions while capturing \emph{uncertainty with rich temporal evolution}.
In \cref{chap:reinforcement_learning}, we present the first framework to disentangle aleatoric and epistemic uncertainty estimation in reinforcement learning. This framework proposes explicit desiderata, four models inspired from supervised learning, and a detailed experimental setup which cover aleatoric and epistemic uncertainty estimation at both \emph{training time} and \emph{testing time}.

In \cref{part:conclusion}, we present a retrospective on the evolution of the research field since our first study (see \cref{chap:retrospective}) and a conclusion including open questions as suggestion to future research directions (see \cref{chap:conclusion}).

\begin{table}
    \caption{List of own publications that this thesis is based on. Code and datasets for the respective publications are available at \texttt{www.cs.cit.tum.de/daml/[project]}.}
    \vspace{2mm}
    \label{tab:publications}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{lllll}
      {Ch.} & {Ref.} & {Title} & {Conference} & {Repository}\\
      \midrule
      \ref{chap:classification} & \cite{charpentier2020} & \makecell[l]{Posterior network: Uncertainty estimation without\\ ood samples via density-based pseudo-counts} & NeurIPS 2020 & \texttt{/postnet/}\\
      \ref{chap:regression} & \cite{NatPN2021} & \makecell[l]{Natural posterior network: Deep bayesian predictive\\ uncertainty for exponential family distributions.} & ICLR 2021 & \texttt{/natpn/}\\
      \ref{chap:robustness} & \cite{robustness-uncertainty-dirichlet} & \makecell[l]{Evaluating robustness of predictive uncertainty estimation:\\ Are dirichlet-based models reliable?} & ICML 2020 & \texttt{/dbu-robustness/}\\
      \ref{chap:graph_data} & \cite{graph-postnet} & \makecell[l]{Graph posterior network: Bayesian predictive uncertainty\\ for node classification.} & NeurIPS 2021 & \texttt{/graph-postnet/}\\
      \ref{chap:sequential_data} & \cite{uceloss} & \makecell[l]{Uncertainty on asynchronous time event prediction} & NeurIPS 2019 & \texttt{/uncertainty-event-prediction/}\\
      \ref{chap:reinforcement_learning} & \cite{charpentier2022uncertainty-rl} & \makecell[l]{Disentangling epistemic and aleatoric uncertainty\\ in reinforcement learning} & DFUQ - ICML 2022 & \texttt{/aleatoric-epistemic-uncertainty-rl/}\\
    \end{tabular}
    \end{adjustbox}
\end{table}

\section{Own publications}

The content of Chapters \ref{chap:classification} to \ref{chap:reinforcement_learning} is mostly based on papers previously published at international peer-reviewed conferences. We list these papers in Table~\ref{tab:publications}.
We also provide the full list of publications that the author was involved in during the PhD studies below. The name of the author of this thesis is put first in case of equal first authorship. These publications focus on three main topics: \emph{uncertainty estimation} including Bayesian models, energy-based models and robustness \citep{charpentier2020,NatPN2021,robustness-uncertainty-dirichlet,graph-postnet,uceloss,charpentier2022uncertainty-rl,ood_ebm,ayle2022robustness-sparse}, \emph{structure learning} including hierarchical and directed acyclic graphs \cite{charpentier2022dpdag,charpentier2019tsd,zugner2022endtoend,bonald2020scikitnetwork}, and \emph{efficient ML} including pruning methods and sparse neural networks \cite{rachwan2022earlycrop,ayle2022robustness-sparse}.

\renewcommand{\bibsection}{}
\input{sections/own_publications/own_publications.bbl}