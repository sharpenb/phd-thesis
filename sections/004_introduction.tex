\chapter{Introduction}
\label{chap:introduction}

Artificical Intelligence (AI) consists of the intelligence demonstrated by machines in contrast with intelligence demonstrated by humans or animals.
AI includes both Machine Learning (ML) which consists in the study of computer programs which automatically learn from experience \citep{Mitchell97}, and Deep Learning (DL) which is a subfield of ML and can be defined as models which hierarchically learn complex representations based on simpler ones \citep{GoodBengCour16}.
AI is a very important technology with high potential economical and ethical impact. 

Economically, AI has a high momentum. Indeed, 48\% of companies claim that they use AI \cite{ai-market-oreilly} and 83\% companies have AI is their priority \cite{ai-market-forbes}. Overall, the market value of AI is \$136 billion in 2022 \cite{ai-market} and had a Compound Annual Growth Rate (CAGR) of 38.5\% between 2022-2030 \cite{ai-market}, thus indicating a very fast growth.
Hence, AI founds many applications in science and industry. 
Industry applications include e.g. agriculture (e.g. Cropin, Iron Ox, FarmWise), manufacturing (e.g. Exotec, Bright Machines, Fieldbox), automotive industry (e.g. Tesla, Waymo), medicine (e.g. Exscientia, Valence Discovery, Radium), construction (Procore Technologies, OpenSpace), finance (e.g. Affirm, Kreditech, Kayrros), education (e.g. Duolingo, Age of Learning), or even for art (e.g. Bytedance, OpenAI).
Scientific applications include e.g. physics (e.g. \cite{gao2022abinitio}), chemistry (e.g. \cite{huang2021abinitio}), biology (e.g. \cite{hetzel2022predicting}), or even math (e.g. \cite{cobbe2021math}).
Hence, AI have shown to be applicable in various data types (e.g. images, tabular, text, graphs) and at various scales from very small (e.g. molecules) to very large systems (e.g. social networks).

Ethically, AI systems demonstrate multiple important concerns.
It does not guarantee a safe behavior and can create accidents \cite{amodei2019problems}.
It does not provide clear explanations for its predictions or provide unreliable explanations \cite{overview-interpretable-ml,interpretable-ml, krishnan2020against}.
It might be racist and discriminate minorities \cite{mehrabi2019fairness}.
It can partially replace human jobs by stealing human creations which leaves the question of intellectual property unclear \cite{moerland2022intellectual, gervais2020intellectual}.
Finally, it might have goals which are not aligned with human goals. This is particularly problematic when AI systems achieve a super-intelligence \cite{bostrom2014superintelligence} which is realistic in many task where AI systems are already better than humans \cite{firestone2020performance}.

The fast AI economic growth and the multiple AI ethical concerns urge the development of reliable AI models which is the main subject of this thesis.

\section{Why do we need uncertainty ?}

Uncertainty estimation (a.k.a. uncertainty quantification) consists in evaluating the confidence of models in their predictions. This task is crucial for both practical and theoretical reasons:

\paragraph*{Practical reasons.} The Dunning-Kruger effect \cite{dunning-kruger} describes a psychological cognitive bias in which people lacking knowledge in a particular domain overestimate their abilities. Interestingly, this phenomenon also applies to machine learning models. Traditional neural networks show overconfident predictions, in particular on data that is different from the data seen during training \cite{overconfident-relu}. The illusion of knowledge of machine learning models highly impacts the reliability of such  models  in  safety-critical  domains.
First, it affects the \emph{trust} in ML model predictions. Ideally, we expect ML models to be confident when predicting correctly and uncertain when predicting wrongly. 
Second, it affects the \emph{safety} of ML predictions in unfamiliar situations. Ideally, we expect ML models to flag predictions on unknown domains corresponding to anomaly detection.
Third, it affects the \emph{maintenance} of ML models. Ideally, we expect ML models to become more uncertain when the testing data has drifted away from the training data, thus indicating the need of retraining the model. This is particularly important in application where ML models need to explore and learn all life-long.
Finally, it affects the \emph{fairness} of ML models. Ideally, we expect ML models to provide calibrated predictions on all input regions including underrepresented data.
A summary of the practical reasons and their associated use cases is given in table \bc{Do table}.

\paragraph*{Theoretical reasons.} ML models aim at precisely representing the flow of information in the world which is inherently uncertain.
The world is \emph{non-deterministic at a large scale}. Indeed, uncertainty emerges when small individual events contribute to macro phenomena like GDP growth, micro phenomena like the growth rate of firms, and non-economic events like war and climate change \cite{macro-micro-uncertainty}.
The world is \emph{non-deterministic at a small scale}. Indeed, the uncertainty principle in (quantum) physics implies that it is in general not possible to predict the value of a particle quantity (like position and speed) with arbitrary certainty, even if all initial conditions are specified \cite{hilgevoord2016heisenberg}.
The world is only \emph{partially observable}. Indeed, agents like humans or robots have to deal with incomplete information on the environment in which they evolve \cite{kaelbling1998pomdp}. At the extreme, since the speed of information is limited by the speed of light, accessible information is restricted to the observable world, thus making the world inherently uncertainty for any agent \cite{ord2021universe}.
A summary of the theoretical reasons is given in table \bc{table}.

\paragraph*{} All these reasons underline the need of accurate uncertainty estimation methods in ML. 
Specifically, a reliable ML model should provide high-quality estimates of \emph{aleatoric} and \emph{epistemic} uncertainty \citep{uncertainty-deep-learning}.
The aleatoric uncertainty allows a model to account for the irreducible data uncertainty (e.g. the inherent sensor noise, or the inherent environment stochasticity).
The epistemic uncertainty allows a model to account for its lack of knowledge about unseen data regions (e.g. testing data differs significantly from training data).
Aleatoric and epistemic uncertainty levels can eventually be combined into an overall \emph{predictive} uncertainty \citep{uncertainty-deep-learningout}. Hence, this thesis studies the usage of different types of uncertainty for ML methods.

\section{Why do we need independent and non-independent data ?}

In this thesis, we consider ML models which process input data to accurately predict output targets. 
More specifically, we expect ML models to be able to process \emph{any type of input data} (e.g. tabular, images, graph or time series) to predict \emph{any type of output data} (e.g. classes, continuous values, counts).
To this end, \emph{independence} and \emph{non-independence} are key assumptions to create \emph{practical} and \emph{accurate} models which precisely describe the real-world. 

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[t]{0.245 \columnwidth}
        \centering
        $\vcenter{\hbox{\includegraphics[width=0.9 \textwidth]{resources/tabular-cropped.pdf}}}$
         \caption{Tabular data}
         \label{fig:tabular_data}
    \end{subfigure}%
    \begin{subfigure}[t]{0.245\columnwidth}
        \centering
        $\vcenter{\hbox{\includegraphics[width=0.9 \textwidth]{resources/image-cropped.pdf}}}$
        \caption{Image data}
        \label{fig:image_data}
    \end{subfigure}%
    \begin{subfigure}[t]{0.245 \columnwidth}
        \centering
        $\vcenter{\hbox{\includegraphics[width=0.9 \textwidth]{resources/graph-cropped.pdf}}}$
         \caption{Graph data}
         \label{fig:graph_data}
    \end{subfigure}%
    \begin{subfigure}[t]{0.245\columnwidth}
        \centering
        $\vcenter{\hbox{\includegraphics[width=0.9 \textwidth]{resources/sequential-cropped.pdf}}}$
        \caption{Sequential data}
        \label{fig:sequential_data}
    \end{subfigure}%
    \caption{Overview of different data types covering independent $\x_i$ inputs like tabular and image data, and dependent $\x_i$ inputs like graph node and sequential time events.}
	% \vspace{-.5cm}
\end{figure}

\paragraph*{Independent Data.} The independence assumption means that different data samples are not connected in any way. In other words, the knowledge of one data sample does not bring any information on another data sample.
The independence assumption is particularly useful when representing tabular data \cite{raschka2022chronology} (e.g. a group of unrelated persons for a medicine trial, defects on multiple different devices) or image data \cite{lu2020survey, litjens2017survey} (e.g. disease detection on medical images of different patients, object detection in different self-driving cars or robots). Visualizations of such data are represented in Fig.~\ref{fig:tabular_data} and Fig.~\ref{fig:image_data}.
In this case, representing the interaction between data samples does not bring much information to perform the predictions.
Hence, the key advantage of the independence assumption is that it allows mathematical factorization for practical modelling simplifications \citep{bishop} without important information loss.

\paragraph*{Non-Independent Data.} The non-independence assumption means that different data samples might be connected in some way. In other words, observing a data sample gives some information on the value of another data sample.
The non-independence assumption is particularly useful when representing graph data \cite{GNNBook2022} (e.g. social networks, citation networks) or sequential data \cite{shchur2021review, Dietterich2002Machine} (e.g. financial time series, interaction history of a user). Visualizations of such data are represented in Fig.~\ref{fig:graph_data} and Fig.~\ref{fig:sequential_data}.
In this case, neighboring nodes of a graph are expected to share important information and past events are expected to give important information on future events.
Hence, the key advantage of the non-independence assumption is that it retains the information contained in graph and sequential interactions to model the world more precisely.

\paragraph*{} Beyond the \emph{(non-)independence assumption}, another common assumption in ML is that data are \emph{identically distributed}. 
This assumption assumes that all input data comes from the same data distribution which also has strong limitations related to the reasons motivating the usage of uncertainty estimates for ML predictions.
While the training data are assumed to come from the same data distribution, the testing data might come from different distributions and suffer from \emph{Distribution Shifts} \cite{rabanser2019shift, dataset-shif}. 
Indeed, the testing data might come from the \emph{In-Distribution (ID)} similar to the training data, or a \emph{Out-Of-Distribution (OOD)} which could be any distribution different from the training distribution \cite{ood-detection-survey, shen2021ood}.
This scenario frequently happens in \emph{safety} and \emph{maintenance} use-cases where the data distribution observed by the model continuously drifts at testing time. 

Hence, this thesis acknowledges the limitation of the standard \emph{independent and identically distributed (i.i.d.)} assumptions.
To this end, this thesis studies the application of uncertainty estimation to independent and non-independent data.

\section{Contributions and outline}

This thesis studies the application of uncertainty estimation for independent and non-independent data via three main components:
\begin{itemize}
    \item Explicit \emph{desiderata} capturing the desired behavior of uncertainty estimation.
    \item Accurate \emph{models} for uncertainty estimation with low practical overhead.
    \item Practical \emph{experimental setup} evaluating uncertainty estimation in real-world applications including worst-case scenarios.
\end{itemize} 

To this end, we start by establishing the background knowledge about uncertainty estimation in Chapter~\ref{chap:background}.

In Part~\ref{part:independent_data}, we present a study of uncertainty estimation for \emph{independent data}: 
In Chapter~\ref{chap:classification}, we construct a new Bayesian model for uncertainty estimation for classification called \PostNet{} (\PostNetacro{}). \PostNetacro{} requires a \emph{single-forward pass}, does need \emph{no OOD data during training}, and adapt to \emph{many core architectures}.
In Chapter~\ref{chap:regression}, we construct a new Bayesian model for uncertainty estimation for regression called \NatPN{} (\NatPNacro{}). Beyond regression, \NatPNacro{} applies to a \emph{large variety of supervised tasks} (incl. classification and count prediction) with \emph{very low computational overhead}.
In Chapter~\ref{chap:robustness}, we present the first study of the robustness of uncertainty models in the worst-case scenario of adversarial attacks. We show that uncertainty estimates of an important family of single-forward pass models are \emph{not robust} for many important real-world tasks (incl. correct/wrong prediction detection, adversarial examples detection, and ID/OOD detection). Further, we explore first approaches methods to improve uncertainty robustness by using \emph{adversarial training} and \emph{randomized smoothing}. 

In Part~\ref{part:non_independent_data}, we present a study of uncertainty estimation for \emph{non-independent data}:
In Chapter~\ref{chap:graph_data}, we present the first framework for uncertainty estimation for node classification on graph data. This framework proposes explicit desiderata, a new Bayesian model, and an exhaustive evaluation setup which cover aleatoric and epistemic uncertainty estimation \emph{without} and \emph{with network effects}.
In Chapter~\ref{chap:sequential_data}, we present new models for uncertainty estimation in sequential data with asynchronous time events. The two proposed models are able of accurate \emph{event types} and \emph{event time} predictions while capturing \emph{uncertainty with rich temporal evolution}.
In Chapter~\ref{chap:reinforcement_learning}, we present the first framework to disentangle aleatoric and epistemic uncertainty estimation in reinforcement learning. This framework proposes explicit desiderata, four models inspired from supervised learning, and a detailed experimental setup which cover aleatoric and epistemic uncertainty estimation at both \emph{training time} and \emph{testing time}.

In Part~\ref{part:conclusion}, we present a retrospective on the evolution of the research field since our first study (see Chapter~\ref{chap:retrospective}) and a conclusion including open questions as suggestion to future research directions (see Chapter~\ref{chap:conclusion}).

\begin{table}
    \caption{List of own publications that this thesis is based on. Code and datasets for the respective publications are available at \texttt{www.cs.cit.tum.de/daml/[project]}.}
    \vspace{2mm}
    \label{tab:publications}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{lllll}
      {Ch.} & {Ref.} & {Title} & {Conference} & {Repository}\\
      \midrule
      \ref{chap:classification} & \cite{charpentier2020} & \makecell[l]{Posterior network: Uncertainty estimation without\\ ood samples via density-based pseudo-counts} & NeurIPS 2020 & \texttt{/postnet/}\\
      \ref{chap:regression} & \cite{NatPN2021} & \makecell[l]{Natural posterior network: Deep bayesian predictive\\ uncertainty for exponential family distributions.} & ICLR 2021 & \texttt{/natpn/}\\
      \ref{chap:robustness} & \cite{robustness-uncertainty-dirichlet} & \makecell[l]{Evaluating robustness of predictive uncertainty estimation:\\ Are dirichlet-based models reliable?} & ICML 2020 & \texttt{/dbu-robustness/}\\
      \ref{chap:graph_data} & \cite{graph-postnet} & \makecell[l]{Graph posterior network: Bayesian predictive uncertainty\\ for node classification.} & NeurIPS 2021 & \texttt{/graph-postnet/}\\
      \ref{chap:sequential_data} & \cite{uceloss} & \makecell[l]{Uncertainty on asynchronous time event prediction} & NeurIPS 2019 & \texttt{/uncertainty-event-prediction/}\\
      \ref{chap:reinforcement_learning} & \cite{charpentier2022uncertainty-rl} & \makecell[l]{Disentangling epistemic and aleatoric uncertainty\\ in reinforcement learning} & DFUQ - ICML 2022 & \texttt{/aleatoric-epistemic-uncertainty-rl/}\\
    \end{tabular}
    \end{adjustbox}
\end{table}

\section{Own publications}

The content of Chapters \ref{chap:classification} to \ref{chap:reinforcement_learning} is mostly based on papers previously published at international peer-reviewed conferences. We list these papers in Table~\ref{tab:publications}.
We also provide the full list of publications that the author was involved in during the PhD studies below. These publications focus on three main topics: \emph{uncertainty estimation} (incl. bayesian models, energy-based models and robustness), \emph{structure learning} (incl. hierarchical and directed acyclic graphs), and \emph{efficient ML} (incl. pruning methods and sparse neural networks).

\renewcommand{\bibsection}{}
\input{sections/own_publications/own_publications.bbl}