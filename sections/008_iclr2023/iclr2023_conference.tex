\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}
\iclrfinalcopy

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{pgfplotstable}
\usepackage{booktabs}
\usepackage{xspace}
\usepackage{wrapfig}
\usepackage{cleveref}
\usepackage{colortbl}
\usepackage{bm}
\usepackage{tabularx}

\definecolor{mycolor}{RGB}{220,220,220}
\renewcommand\theadfont{}
% \usepackage{minipage}
% \pgfplotsset{compat=1.18}

% \title{AN EMPIRICAL ANALYSIS OF TECHNIQUES TO IMPROVE DETERMINISTIC UNCERTAINTY METHODS}
\title{Training, Architecture, and Prior \\for Deterministic Uncertainty Methods}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{%
  Bertrand Charpentier, Chenxiang Zhang, Stephan GÃ¼nnemann \\
  Technical University of Munich, Germany\\
  \texttt{\{charpent,zch,guennemann\}@in.tum.de} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document} 


\maketitle

\begin{abstract}
Accurate and efficient uncertainty estimation is crucial to build reliable Machine Learning (ML) models capable to provide calibrated uncertainty estimates, generalize and detect Out-Of-Distribution (OOD) datasets. To this end, Deterministic Uncertainty Methods (DUMs) is a promising model family capable to perform uncertainty estimation in a single forward pass. This work investigates important design choices in DUMs: 
\textbf{(1)} we show that \emph{training} schemes decoupling the core architecture and the uncertainty head schemes can significantly improve uncertainty performances. 
\textbf{(2)} we demonstrate that the core \emph{architecture} expressiveness is crucial for uncertainty performance and that additional architecture constraints to avoid feature collapse can deteriorate the trade-off between OOD generalization and detection. 
\textbf{(3)} Contrary to other Bayesian models, we show that the \emph{prior} defined by DUMs do not have a strong effect on the final performances.
\end{abstract}

\input{sections/introduction}
% \input{sections/related_work}
\input{sections/method}
\input{sections/experiments/training.tex}
\input{sections/experiments/encoder.tex}
\input{sections/experiments/prior.tex}
\input{sections/conclusion}

\newpage
\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\newpage
\input{sections/appendix}

\end{document}
