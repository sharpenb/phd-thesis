\section{Introduction}
%\label{sec:introduction_008}

% \looseness=-1
% Safety is critical to the adoption of deep learning in domains such as autonomous driving, medical diagnosis, or financial trading systems. A solution for this problem is to create reliable models capable to estimate the uncertainty of its own predictions. 
% Different uncertainty types are divided in \textit{aleatoric} uncertainty quantified by the inherited noise in the data, thus irreducible; \textit{epistemic} uncertainty quantified by the modeling choice or lack of data, thus reducible; \textit{predictive} uncertainty, a combination of aleatoric and epistemic \citep{gal2016uncertainty}. 
% In practice, high quality uncertainty estimates must be calibrated and able to detect Out-Of-Distribution (OOD) data like anomalies while preserving good Out-Of-Distribution (OOD) generalization performances like on dataset shifts.

While we have focused in the previous sections on proposing new Bayesian models for efficient uncertainty estimation on independent data, we now turn our attention on the practical considerations when using efficient uncertainty estimation methods. 

Recently, a family of methods for uncertainty estimation named Deterministic Uncertainty Methods (DUMs) have emerged \citep{postels2022practicalitydum}. 
Contrary to uncertainty methods such as Ensembles \citep{ensembles}, MC Dropout \citep{dropout} or other Bayesian neural networks on weights \citep{bayesian-networks}, which require multiple forward passes to make predictions, DUMs only require a single forward pass, thus making them significantly more computationally efficient. 
%These models can make predictions in only a single forward pass,  %thus being computationally efficient. 
Generally, DUMs are composed of three components with high potential impact on their performances: the \emph{training} procedure which is supposed to optimize the model toward high predictive and uncertainty performances,  the core \emph{architecture} which is supposed to define informative embeddings used to make predictions, and the \emph{prior} which is supposed to define the default uncertain predictions. In this work, we investigate the role of these three components on the quality of DUMs uncertainty estimates by evaluating calibration performances, OOD detection, and OOD generalization. Our main contributions are:
\vspace{-2mm}
\begin{itemize}
%\setlength\itemsep{-1mm}
    \item \textbf{Training}: We show that \emph{decoupling the learning rates} of the core architecture and uncertainty heads of DUMs, \emph{jointly training} the core architecture and the uncertainty head of DUMs, and \emph{pretraining} with \emph{more data} and \emph{higher data quality} improve uncertainty performances. 
    \item \textbf{Architecture}: We demonstrate that the expressiveness of the core architecture defined by the \emph{architecture type}, \emph{architecture size}, and \emph{dimension of the latent space} is crucial for \emph{both} predictive and uncertainty performances. Further, we show that applying additional regularization constraints to avoid \emph{feature collapse} does not find better trade-off between OOD detection and generalization, even sometimes degrading performances.
    \item \textbf{Prior}: In contrast to Bayesian neural networks on weights where the choice of prior is critical \citep{bayesposterior2020wenzel, fortuin2022prior, noci2021prior_cpe, kapoor2022prior_cpe}, we empirically show that the choice of prior defined in the training loss or in the uncertainty head of DUMs has a relatively small effect on the final uncertainty performances.
\end{itemize}
