\chapter{Conclusion}
\label{chap:conclusion}

\section{Reliable ML beyond uncertainty estimation}

\paragraph{Robustness.} Robustness is important in ML to guarantee that model predictions do not change when imperceptible perturbations are applied to the input data. This covers empirical robustness and certifiable robustness against both natural and adversarial perturbations \cite{tu2020empirical, chun2020empirical, cohen2019, zugner2020certifiable}. Robustness is a very active field of research with many studies on robustness for independent data \cite{silva2020opportunies} but also for dependent data like graph data \cite{GNNBook-ch8-gunnemann} or sequential data \cite{cheng2020}.

\paragraph{Interpretability.} Interpretability is important in ML to explain the reasons for model predictions. It ensures impartial decision-making and indicate if meaningful variables are used to infer the prediction output. It covers different types of explanations like text explanations, visual explanations, local explanations or explanations by example. Interpretability and explainability are also very active fields with many existing studies \cite{arrieta2019explainable, overview-interpretable-ml}

\paragraph{Privacy.} Privacy consists in preventing an attacker to infer facts about members of a population, about data in the training set, or about the model parameters \cite{cristofaro2020privacy, cristofaro2021privacy}. Privacy is important to avoid data leakage in both centralized and federated learning. An important defense against privacy attackers is differential privacy \cite{buglesi2006privacy} which tried to learn useful information from the whole training dataset without retaining specific information about individual samples.

\paragraph{Green AI.} Green AI consists in models with low CO2 emissions and low energy consumption which is key to propose sustainable AI. Green AI is particularly important since computations required for deep learning research have been doubling every few months \cite{schwartz2019greenAI}. Existing techniques to reduce required computations for ML models include quantization, distillation, and model pruning \cite{neill2020compression}. 

\paragraph{AI alignment.} AI alignment aim to align the intended goals of the AI designers,  the specified goals to the AI system, and the emergent goals that the AI system actually advances \cite{bostrom2014superintelligence, gabriel2020AI}. This field is still at its infancy. Recent works on AI alignment involve large foundational models (e.g. \cite{gpt, rombach2021highresolution, galactica}) which might show misaligned behavior which can be (partially) realigned similarly to InstructGPT \cite{instructgpt} or ChatGPT \cite{chatgpt}.

\section{Broader Impact}

Accurate uncertainty estimation aims at improving trust in safety-critical domains subject to automation, and in a maintenance context where the underlying data distribution might slowly shift over time. In this regard, this thesis significantly improves the applicability of uncertainty estimation across a wide range of input (e.g. tabular, images, graph data, sequential data, etc) and output domains (e.g. classification, regression, count prediction, etc) while maintaining a fast inference time. This could be particularly beneficial in industrial applications with time pressure and potential critical consequences (e.g. finance, medicine, policy decision-making, etc).

Nonetheless, while methods described in this thesis achieve high-quality uncertainty estimation, there is always a risk that they do not fully capture the real-world complexity e.g. for OOD data close to ID data. Furthermore, we raise awareness about two other risks of excessive trust related to the \emph{Dunning-Kruger effect} \citep{dunning-kruger}: human excessive trust in Machine Learning model capacity, and human excessive trust in its own interpretation capacity. Therefore, we encourage practitioners to proactively confront the model design and its uncertainty estimates to desired behaviors in real-world use cases.

\section{Open Questions}

Beyond the potential improvements mentioned in \cref{chap:retrospective}, we identified other open questions related to uncertainty estimation in ML.

\paragraph{How to estimate uncertainty for large foundational models?} Recently, many large foundational models have been developed (e.g. \cite{gpt, rombach2021highresolution, galactica}) with already high impact on real-world applications like art creation, text writing, coding, or scientific research. It is important to develop uncertainty methods adapted to these methods to avoid misusage of their (potentially wrong) predictions. Nonetheless, we still do not have full solutions to assess the uncertainty of large foundational models especially in the presence of misaligned goals.

\paragraph{How to continually learn in the presence of uncertainty?} Intuitively, uncertainty is expected to help to continually learn on a series of tasks (e.g. \cite{ebrahimi2020uncertainty-guided,khan2021knowledge,farquhar2018towards}) or actively select useful data for training (e.g. \cite{jain2022biological,gal2017bald,kirsch2019batch,kirsch2021simple,tata2022can}). Indeed, low uncertainty should indicate already learned inputs, while high uncertainty should indicate unknown data regions. However, the study of uncertainty estimation to achieve state-of-the-arts performance is still an important research direction.

\paragraph{Why is the model uncertain?} Uncertainty estimates on the predictions might be sensitive to small input data perturbations (see \cref{chap:robustness}) which makes unclear whether uncertainty predictions are always based on valid reasons. Hence, it is crucial to have reliable explanations on the causes of the uncertainty predictions. However, apart from \cite{antoran2021getting} which proposed a first method to interpret uncertainty estimates, there have been very few works on generating explanations and understanding the causes of uncertainty estimates. 

\paragraph{How to estimate uncertainty on inferred causal relationships?} Causal inference aims at inferring causal relationships from observed data. However, the observed data might not be sufficient to have a clear conclusion on the causal relationship between two variables, thus making them non-identifiable \cite{pearl2009causality}. Only very few works defined distributions on DAGs to provide uncertainty estimates \cite{charpentier2022dpdag}. Hence, the area of uncertainty estimation for causal inference is still underexplored.

\epigraph{I know one thing, that I know nothing.}{\textit{Socrates}}

% \epigraph{The only true wisdom is in knowing you know nothing.}{\textit{Socrates}}

\epigraph{La connaissance progresse en int√©grant en elle l'incertitude, non en l'exorcisant}{\textit{Edgar Morin}}

\epigraph{Madness is the consequence not of uncertainty but of certainty.}{\textit{Friedrich W. Nietzsche}}

\epigraph{To know what you know and what you do not know, that is true knowledge}{\textit{Confucius}}

\epigraph{Uncertainty is the only certainty there is, and knowing how to live with insecurity is the only security.}{\textit{John Allen Paulos}}

\epigraph{As far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.}{\textit{Albert Einstein}}

\epigraph{I can live with doubt and uncertainty and not knowing. I think it is much more interesting to live not knowing than to have answers that might be wrong.}{\textit{Richard P. Feynman}}

\epigraph{Uncertainty that comes from knowledge (knowing what you don't know) is different from uncertainty coming from ignorance.}{\textit{Isaac Asimov}}

\epigraph{Doubt is not a pleasant condition, but certainty is absurd.}{\textit{Voltaire}}

\epigraph{The more I see the less I know for sure.}{\textit{John Lennon}}

\epigraph{Ignorance more frequently begets confidence than does knowledge.}{\textit{Charles Darwin}}

\epigraph{Science is the outcome of being prepared to live without certainty and therefore a mark of maturity. It embraces doubt and loose ends.}{\textit{A.C. Grayling}}
