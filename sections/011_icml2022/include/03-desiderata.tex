\vspace{-3mm}
\section{Desiderata for Uncertainty Quantification in RL}
\label{sec:desiderata_011}

\looseness=-1
In this section, we \emph{explicitly} define four intuitive and general desiderata that capture the desired behavior for uncertainty estimates in our RL setup. The desiderata cover \emph{aleatoric} and \emph{epistemic} uncertainty at both \emph{training} and \emph{testing} time. The distinction between aleatoric and epistemic uncertainty is commonly studied in SL \citep{uncertainty-deep-learning, PriorNetworks, natpn}. In contrast, RL mostly focuses on accounting for aleatoric uncertainty with risk-sensitive policy or distributional RL to achieve higher returns \citep{distributional-rl-prespective, distributional-rl, iqn}. We design the desiderata to be informal and generic for two main reasons. First, it allows them to intuitively indicate the expected behavior of the uncertainty estimation without complex mathematical notations \citep{graph-postnet,Eswaran2017}. Second, it makes the desiderata application independent and model-agnostic.

The second distinction differentiates between training and testing time relevant to sample efficiency and generalization in RL. In contrast, SL mostly focuses on testing time performance.

\looseness=-1
\textbf{Training Time.} We describe the desired behavior of uncertainty estimates at training time. First, we describe the desired uncertainty behavior when observing more samples of the training environment.
\begin{desiderata}
    \label{ax:training_state}
    An agent interacting longer with one specific environment at training time should become more epistemically confident when predicting actions on this same environment.
    %An agent training longer on states sampled from one specific environment should become more epistemically confident when predicting actions on states sampled from the same specific environment.
\end{desiderata}
\vspace{-2mm}
Intuitively, an agent observing more samples from the same environment distribution should accumulate more knowledge through time, thus being more epistemically certain. In practice, des.~\ref{ax:training_state} expresses that the epistemic uncertainty estimates should reflect the accumulated knowledge, and thus the convergence, of the agent during training. We test des.~\ref{ax:training_state} in the experiments by tracking the epistemic uncertainty at training time (see \cref{sec:experiments_011}). Second, we describe the behavior of the total reward when selecting actions based on uncertainty estimates at training time.
\begin{desiderata}
    \label{ax:training_strategy}
    All else being equal, an agent selecting actions with the sampling-aleatoric strategy at training time should achieve lower sample efficiency than an agent selecting actions with the sampling-epistemic strategy.
\end{desiderata}
Intuitively, an agent exploring states with more (irreducible) aleatoric uncertainty would gain less knowledge about the environment dynamic than an agent exploring states with high epistemic uncertainty where the agent lacks knowledge. However, there is an important trade-off between over- or under-exploring epistemically uncertain actions which could lead to lower sample efficiency. The sampling-epistemic strategy, which corresponds to Thompson sampling \citep{thompson-sampling}, mitigates this exploration-exploitation problem by sampling action w.r.t. the epistemic distribution. Thompson sampling has already \emph{empirically} demonstrated high sample efficiency in deep RL problems \citep{dropout} and provably achieve low regret in many decision-making problems like multi-arm Bandits \citep{thompson-sampling-mab, thompson-sampling-information}. In practice, des.~\ref{ax:training_strategy} suggests that an agent should use the sampling-epistemic strategy for a better exploration-exploitation trade-off. We test des.~\ref{ax:training_strategy} in the experiments by comparing the sample efficiency of the sampling-aleatoric and the sampling-epistemic strategies during training (see \cref{sec:experiments_011}).

\looseness=-1
\textbf{Testing Time.} We describe the desired behavior of uncertainty estimates at testing time. First, we describe the desired uncertainty behavior when observing samples from an environment different from the training environment.
\begin{desiderata}
    \label{ax:testing_state}
    At testing time, epistemic uncertainty should be greater when the agent interact with environments that are very different from the original training environments.
\end{desiderata}
\vspace{-2mm}
The environment difference can be measured using different distance metrics depending on the task or application requirements \citep{domain-shifts-rl}. Intuitively, an agent should be less confident when observing new states at test time that were not used to collect knowledge at training time. In practice, des.~\ref{ax:testing_state} suggests that an agent should be able to use epistemic uncertainty estimates to detect states that are abnormal compared to the states observed during training. We test des.~\ref{ax:testing_state} in the experiments by comparing the epistemic uncertainty of the training environment against the uncertainty in noisy environments at testing time (see \cref{sec:experiments_011}). Noisy environments include environments with completely random states, and environments with different strengths of perturbation on the original states, actions, or transition dynamics. Depending on the application, different noise or perturbation types might be used to characterize the difference of the testing environment with the training environment. Second, we describe the behavior of the total reward when selecting at testing time actions based on uncertainty aleatoric or epistemic uncertainty estimates.
\begin{desiderata}
    \label{ax:testing_strategy}
    All else being equal, an agent sampling actions from the epistemic uncertainty at training and testing time should generalize better at testing time than an agent sampling actions from the aleatoric uncertainty.
\end{desiderata}
\vspace{-2mm}
Intuitively, an agent exploring more epistemically uncertain states at training time would collect more knowledge about the environment, thus generalizing to more states at testing time. Further, since the environment dynamic is not directly observed, an agent should account for the epistemic certainty on the current state to take actions that generalize better at testing time. In particular, it has been shown that the Bayes-optimal Markovian policy at testing time is stochastic in general due to the partially observed MDP dynamic sometimes called epistemic POMDP \citep{epistemic-pomdp}. In practice, des.~\ref{ax:testing_strategy} suggests that an agent should use the sampling-epistemic strategy for more robust generalization performance. We test des.~\ref{ax:testing_strategy} in the experiments (see \cref{sec:experiments_011}) by comparing the reward obtained by the sampling-aleatoric and the sampling-epistemic strategies at testing time. The two latter desiderata \ref{ax:testing_state} and \ref{ax:testing_strategy} express an important trade-off between assigning high uncertainty and generalizing to new test environments. Since an agent cannot generalize to all new environments because of the No Free Lunch Theorem \citep{no-free-lunch-theorem-optimization}, an agent should assign higher uncertainty to environments where it does not generalize. We jointly test des.~\ref{ax:testing_state} and des.~\ref{ax:testing_strategy} in the experiments (see \cref{sec:experiments_011}) by tracking the reward and the uncertainty estimates in test environments with different perturbation strengths.
