\vspace{-3mm}
\section{Problem Setup}
\label{sec:setup_011}

\paragraph{Uncertainty in SL.} The objective of SL is to accurately predict the output \smash{$\y\dataix$} given an input \smash{$\x\dataix$} with index \smash{$\idata$}. It differentiates between two types of uncertainty: the uncertainty on the label prediction \smash{$\y\dataix$} described by the \emph{aleatoric distribution} \smash{$\prob(\y\dataix \mid \expparam\dataix)$} with parameters \smash{$\expparam\dataix$} estimated from the input $\x\dataix$, and the uncertainty on the predicted label distribution parameters $\expparam\dataix$ described by the \emph{epistemic distribution} \smash{$\prior(\expparam\dataix \mid \priorparam\dataix)$} with parameters $\priorparam\dataix$ estimated from the input $\x\dataix$. Intuitively, the variations of the aleatoric distribution will be high when the input \smash{$\x\dataix$} does not provide discriminative information to determine the label \smash{$\y\dataix$}. The variations of the epistemic distribution will be high when the input \smash{$\x\dataix$} does not provide enough information to determine the label distribution \smash{$\prior(\expparam\dataix \mid \priorparam\dataix)$} described by the parameters \smash{$\priorparam\dataix$}. Thus, the aleatoric uncertainty can be measured in practice by computing the entropy, i.e \smash{$u_\text{alea}(\x\dataix)=\entropy(\prob(\y\dataix \condition \expparam\dataix))$}, or the variance, i.e. \smash{$u_\text{alea}(\x\dataix)= \variance(\prob(\y\dataix \condition \expparam\dataix))$}, of the aleatoric distribution, while the epistemic uncertainty can be measured by computing the entropy, i.e. \smash{$u_\text{epist}(\x\dataix)=\entropy(\prior(\expparam\dataix \mid \priorparam\dataix)$}, of the epistemic distribution \citep{PriorNetworks, charpentier2020, natpn}. Further, SL also distinguishes between sampling-based which often require multiple forward passes for uncertainty estimation, and sampling-free methods which often require a single forward pass for uncertainty estimation. Sampling-based methods like MC drop-out \citep{dropout} and ensemble \citep{ensembles, hyper-ensembles, batch-ensembles} estimate uncertainty by aggregating statistics (e.g. mean and variance) from different samples which \emph{implicitly} describe the epistemic distribution \smash{$\prior(\expparam\dataix \mid \priorparam\dataix)$}. Sampling-free methods like deep kernel learning models \citep{simple-baseline-uncertainty, due, duq, uceloss} and evidential networks \citep{charpentier2020, PriorNetworks, natpn, evidential-regression} estimate uncertainty by \emph{explicitly} parametrizing the epistemic distributions \smash{$\prior(\expparam\dataix) \mid \priorparam\dataix)$} with known distributions such as Normal and Normal Inverse-Gamma (NIG) distributions, thus enabling efficient and closed-form computation of the distribution statistics (e.g. mean, variance or entropy).

\textbf{Uncertainty in RL.} We consider the task of learning RL policies interacting with an environment with fully observed states at every time step $t$. The environment is described by a Markov Decision Process (MDP) $(S, A, R, T, \rho, \gamma)$, where $S$ is the state space, $A$ is the action space, \smash{$R(\s\datatx, a\datatx)$} is the reward associated with the action \smash{$a\datatx$} and state \smash{$\s\datatx$}, \smash{$T(\s^{(t+1)}|\s\datatx, a\datatx)$} is the transition probability, \smash{$\rho(\s^{(0)})$} is the initial state distribution, and $\gamma$ is the discount factor. Given the current state \smash{$\s\datatx$}, our goal is to learn a policy predicting the action \smash{$a\datatx$} leading to the highest discounted return \smash{$Q^{\policy}(\s^{(t)}, a^{(t)}) = R(\s^{(t)}, a^{(t)}) + \gamma \expectation_{T, \policy} [Q^{\policy}(\s^{(t+1)}, a^{(t+1)})]$} (a.k.a. discounted expected reward) in addition to the aleatoric uncertainty \smash{$u_\text{alea}(\s\datatx, a\datatx)$} and the epistemic uncertainty \smash{$u_\text{epist}(\s\datatx, a\datatx)$} on the predicted return. Similar to SL, the aleatoric and epistemic distributions can be instantiated with \smash{$\prob(\y\datatx \mid \expparam\datatx)$} and \smash{$\prior(\expparam\datatx \mid \priorparam\datatx)$} where the predicted value is the future return i.e. \smash{$\y\datatx=Q^{\policy}(\s^{(t)}, a^{(t)})$}. Intuitively, the variation of the aleatoric distribution will be high when the current state $\s\datatx$ and action $a\datatx$ only contains noisy information to determine the future return \smash{$\y\datatx$}, while the epistemic uncertainty will be high when the current state \smash{$\s\datatx$} and action \smash{$a\datatx$} does not provide enough information according to the model to determine the return distribution \smash{$\prior(\expparam\datatx \mid \priorparam\datatx)$} described by the parameters \smash{$\priorparam\datatx$}.  We decide to estimate uncertainty of the return $\y\datatx=Q^{\policy}(\s^{(t)}, a^{(t)})$ for two reasons. First, the return is estimated in many common Deep RL models (e.g. DQN \citep{dqn}, PPO \citep{ppo}, A2C \citep{a2c}). Second, uncertainty of the return implicitly quantifies uncertainty of the optimal action defined by \smash{$a\datatx = \arg\max_{a} Q^{\policy}(\s^{(t)}, a^{(t)})$}. 

Finally, we consider three action selection strategies which is a crucial choice for exploration and generalization in RL: 
\vspace{-2mm}
\begin{itemize}
    \item The \emph{epsilon-greedy} strategy \citep{epsilon-greedy} which selects the action with the highest predicted reward with probability $1-\epsilon$ and samples randomly otherwise.
    \item The \emph{sampling-aleatoric} strategy which takes the action with the highest predicted reward based on one aleatoric distribution sample i.e. \smash{$a\datatx = \arg\max_{a} \y\datatx$} where \smash{$\y\datatx \sim \prob(\y\datatx | \expparam\datatx)$}.
    \item The \emph{sampling-epistemic} strategy which takes the action with the highest predicted reward based on one epistemic distribution sample i.e. $a\datatx = \arg\max_{a} \expectation_{\prob(\y\datatx | \expparam\datatx)}[\y\datatx]$ where \smash{$\expparam\datatx \sim \prior(\expparam\datatx | \priorparam\datatx)$}. This strategy is similar to Thompson sampling \citep{thompson-sampling}.
\end{itemize}
