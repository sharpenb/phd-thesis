\vspace{-3mm}
\section{Models for Uncertainty Quantification in RL}
\label{sec:model_011}

Model-free RL agents commonly rely on learning the expected return \smash{$Q^{\policy}(\s^{(t)}, a^{(t)})$} associated with taking action \smash{$a^{(t)}$} in state \smash{$\s^{(t)}$} and then following a policy $\policy$. It is defined by the Bellman equation:
% \begin{equation}
% \label{eq:bellman_equation}
    $Q^{\policy}(\s^{(t)}, a^{(t)}) = r(\s^{(t)}, a^{(t)}) + \gamma \expectation_{T, \policy} [Q^{\policy}(\s^{(t+1)}, a^{(t+1)})]$.
% \end{equation}
Similarly, the optimal policy \smash{$\policy^*$} achieving the highest expected reward satisfies  the optimal Bellman equation \cite{dynamic-programming}:
\begin{equation}
\label{eq:optimal_bellman_equation}
    Q^{\policy^*}(\s^{(t)}, a^{(t)}) = r(\s^{(t)}, a^{(t)}) + \gamma \expectation_T [\max_{a^{(t+1)}} Q^{\policy^*}(\s^{(t+1)}, a^{(t+1)})]
\end{equation}
However, the exact computation of the optimal $Q$-value is often intractable for large action or state spaces. Therefore, deep RL agents like DQN \cite{dqn}, PPO \cite{ppo} and A2C \cite{a2c} 
aim at approximating the optimal $Q$-value \smash{$Q^{\policy^*}(\s^{(t)}, a^{(t)})$} with a neural network \smash{$f_\theta(\s^{(t)}, a^{(t)})$} with parameter \smash{$\bm{\theta}$}. In particular, DQN enforces eq.~\ref{eq:optimal_bellman_equation} by minimizing the squared temporal difference (TD) error $\|r(\s^{(t)}, a^{(t)}) + \gamma \max_{a^{(t+1)}} f_{\bm{\theta}'}(\s^{(t+1)}, a^{(t+1)}) - f_{\bm{\theta}}(\s^{(t)}, a^{(t)})\|_2$, where \smash{$f_{\bm{\theta}}$} is the learned prediction network and \smash{$f_{\bm{\theta}'}$} is the frozen target network regularly updated with the prediction network parameters during training. The TD error minimization is similar to SL regression with a MSE loss between the prediction \smash{$\hat{\y}\datatx = f_{\bm{\theta}}(\s^{(t)}, a^{(t)})$} and the target \smash{$\y\datatx = r(\s^{(t)}, a^{(t)}) + \gamma f_{\bm{\theta}'}(\s^{(t+1)}, a^{(t+1)})$} with the key difference that the exploration strategy select the data samples that will be used during training.

\looseness=-1
Model-free Deep RL agents often show important limitations for uncertainty estimation because of their neural network architecture choice. For, instance, while DQN only outputs a single scalar representing the mean $Q$-value with no uncertainty estimates, PPO and A2C policies parameterized with standard ReLU networks would provably produce overconfident predictions for extreme input states \cite{overconfident-relu}. In this work, we focus on equipping the widely used DQN RL agent with reliable uncertainty estimates. To this end, we combine DQN with four SL architectures for uncertainty estimation (incl. MC dropout, ensemble, deep kernel learning, and evidential networks) covering a diverse range of sampling-based and sampling-free methods. These four DQN combinations allow to instantiate both aleatoric and epistemic uncertainty with minimal modifications to the training procedure.  We provide a summary of the uncertainty properties of these models in Tab.~\ref{tab:summary_models}.

\begin{table*}[ht!]
    \vspace{-0mm}
	\caption{Summary of the uncertainty properties of the models.}
	\label{tab:summary_models}
	\vspace{-0mm}
	\centering
	\resizebox{.75\textwidth}{!}{%
\begin{tabular}{@{}lcccc@{}}
\toprule
 & DropOut & Ensemble  & Deep Kernel Learning  & Evidential Networks \\
\midrule
Uncertainty concentration (Des.~\ref{ax:training_state}) & \xmark & \xmark & \xmark & \cmark\\
\midrule
Alea. vs epist. sampling at training time (Des.~\ref{ax:training_strategy})  & \cmark & \cmark & \xmark & \cmark\\
\midrule
OOD detection (Des.~\ref{ax:testing_state}) & \xmark & \xmark & \cmark & \cmark\\
\midrule
Alea. vs epist. sampling at testing time (Des.~\ref{ax:training_strategy}) & \cmark & \cmark & \xmark & \cmark\\
\bottomrule
\end{tabular}}
	\vspace{-0mm}
\end{table*}

\looseness=-1
\textbf{MC Dropout.} DQN is combined with MC Dropout \cite{drop_out} in three steps: \textbf{(1)} it samples $K$ independent set of model parameters \smash{$\{\bm{\theta}_k\}_{k=1}^K$} by dropping activations with probability $p$, \textbf{(2)} it performs $K$ forward passes \smash{$\mu_k, \sigma_k = f_{\bm{\theta}_k}(s^{(t)}, a^{(t)})$}, and \textbf{(3)} it aggregates predictions to form the mean prediction \smash{$\mu(s^{(t)}, a^{(t)}) = \frac{1}{K}\sum_{k=1}^K \mu_k$}, the aleatoric uncertainty estimate \smash{$u_\text{alea}(s^{(t)}, a^{(t)}) = \frac{1}{K}\sum_{k=1}^K \sigma_s$}, and the epistemic uncertainty estimate \smash{$u_\text{epist}(s_t, a_t) = \frac{1}{K}\sum_{k=1}^K (\mu_k - \mu(s^{(t)}, a^{(t)}))^2$}. In this case, the aleatoric distribution is Gaussian while the epistemic distribution is implicitly represented by the sampled parameters \smash{$\{\theta_k\}_{k=1}^K$}. Further, the sampling-epistemic strategy is achieved by performing one single forward pass with a single set of sampled model parameters. This is similar to the Thompson sampling strategy used by \citet{dropout}. During training, we train the neural network parameters $\theta$ by using a Gaussian negative log-likelihood loss. The combination of DQN and dropout has been shown to practically improve sample efficiency \cite{dropout}. However, dropout has multiple limitations. First, the dropout uncertainty estimates \emph{provably} do not concentrate with more observed data \cite{randomized-prior-functions}, thus potentially violating des.~\ref{ax:training_state}. Second, there is no guarantee that dropout produce meaningful uncertainty estimates for extreme input states with a finite number of samples $K$, thus potentially violating des.~\ref{ax:testing_state}. Third, dropout might be computationally expensive for large $K$ value since it would require many forward passes for uncertainty estimation.

\looseness=-1
\textbf{Ensemble.} DQN is combined with ensembles \cite{ensembles} in three steps: \textbf{(1)} it trains $K$ independent models with parameters \smash{$\{\theta_k\}_{k=1}^K$}, \textbf{(2)} it performs $K$ forward passes \smash{$\mu_k, \sigma_k = f_{\bm{\theta}_k}(s^{(t)}, a^{(t)})$}, and \textbf{(3)} it aggregates predictions to form the mean prediction \smash{$\mu(\s^{(t)}, a^{(t)}) = \frac{1}{K}\sum_{k=1}^K \mu_k$}, the aleatoric uncertainty estimate \smash{$u_\text{alea}(s^{(t)}, a^{(t)}) = \frac{1}{K}\sum_{k=1}^K \sigma_s$}, and the epistemic uncertainty estimate \smash{$u_\text{epist}(s_t, a_t) = \frac{1}{K}\sum_{k=1}^K (\mu_k - \mu(s^{(t)}, a^{(t)}))^2$}. In this case, the aleatoric distribution is Gaussian while the epistemic distribution is implicitly represented by the parameters of the $K$ networks \smash{$\{\bm{\theta}_k\}_{k=1}^K$}. Further, the sampling-epistemic strategy is achieved by performing one single forward pass with one randomly selected network. This is similar to the Thompson sampling strategy used by \citet{bootstrapped-dqn}. We train the $K$ independent neural network parameters \smash{$\theta_k$} with a Gaussian negative log-likelihood loss. However, ensemble has multiple limitations. First, while the combination of DQN with bootstrapped ensemble and prior functions has been \emph{empirically} shown to improve learning for complex tasks with sparse rewards \cite{bootstrapped-dqn, randomized-prior-functions}, there is no explicit theoretical or empirical evidence that their uncertainty estimates concentrate with more observed data. Second, there is no guarantee that ensembles produce meaningful uncertainty estimates for extreme input states with a finite number of samples $K$, thus potentially violating des.~\ref{ax:testing_state}. Third, ensemble is computationally expensive for large $K$ value since it would require many forward passes and many neural networks. 

\looseness=-1
\textbf{Deep Kernel Learning.} DQN is combined with deep kernel learning \cite{due} in three steps: \textbf{(1)} it predicts one latent representation of each input state i.e. \smash{$\z^{(t)} = f_{\bm{\theta}}(\s^{(t)})$}, and \textbf{(2)} one Gaussian Process per action $a$ defined from a fixed set of $K$ learnable inducing points \smash{$\{\bm{\phi}_{a,k}\}_{k=1}^{K}$} and a predefined positive definite kernel \smash{$\kappa(\cdot, \cdot)$} predicts the mean \smash{$\mu(\s^{(t)}, a)$} and the variance \smash{$\sigma(\s^{(t)}, a)$} of a Gaussian distribution. We train the neural network parameters $\bm{\theta}$ and the inducing points \smash{$\{\bm{\phi}_{a,k}\}_{k=1}^{K}$} jointly with a variational ELBO loss similarly to \cite{due}. In this case, the epistemic distribution is Gaussian \cite{simple-baseline-uncertainty}, i.e. \smash{$u_\text{epist}(s_t, a_t) = \entropy(\DNormal(\mu(\s^{(t)}, a^{(t)}), \sigma(\s^{(t)}, a^{(t)})))$}. Indeed, we show \emph{theoretically} that epistemic uncertainty increases far from training data (see app.~\ref{app:proofs}). Thus, the combination of DQN and deep kernel learning does not suffer from arbitrary uncertainty estimates for extreme input states contrary to ReLU networks \cite{overconfident-relu}. However, one of the limitation of deep kernel learning is that it does not disentangle \emph{aleatoric} and \emph{epistemic} uncertainty. This is similar to deep kernel learning methods in SL \cite{simple-baseline-uncertainty, due, duq}.

\looseness=-1
\textbf{Evidential Networks.} The combination of DQN and the posterior networks \cite{natpn, postnet}  which belong to the class of evidential networks consists in three steps: \textbf{(1)} an encoder $ f_{\bm{\theta}}$ predicts one latent representation of each input state i.e. \smash{$\z^{(t)} = f_{\bm{\theta}}(\s^{(t)})$}, \textbf{(2)} one normalizing flow density estimator \smash{$\prob(. \condition \bm{\omega}_{a})$} and one linear decoder \smash{$g_{\bm{\psi}_{a}}$} per action $a$ predict a Normal Inverse-Gamma distribution \smash{$\prior(\priorparam(\s^{(t)}, a), \evidence(\s^{(t)}, a))$} with parameters \smash{$\priorparam(\s^{(t)}, a) = g_{\bm{\psi}_{a}}(\z^{(t)}, a)$} and \smash{$ \evidence(\s^{(t)}, a) \propto \prob(\z\dataix \condition \bm{\omega}_{a})$}, and \textbf{(3)} it computes the posterior parameters $\priorparam^\text{post}(\s^{(t)}, a) = \frac{\evidence^\text{prior}\priorparam^\text{prior} + \evidence(\s^{(t)}, a)) \priorparam(\s^{(t)}, a)}{\evidence^\text{prior} + \evidence(\s^{(t)}, a))}, \evidence^\text{post}(\s^{(t)}, a)) = \evidence^\text{prior} + \evidence(\s^{(t)}, a)$ where the prior parameters are chosen to enforce high entropy for the prior distribution e.g. \smash{${\priorparam^\text{prior}=(0, 100)^T}, \evidence^\text{prior}=1$} \cite{natpn}. In this case, the epistemic distribution is a Normal Inverse-Gamma distribution and the aleatoric distribution is a Normal distribution \cite{natpn}. We train the neural network parameters $\bm{\theta}$ and $\bm{\psi}$ and the normalizing flow parameters $\bm{\omega}$ jointly with the MSE loss. The entropy of the conjugate prior distribution represents the epistemic uncertainty , i.e. \smash{$u_\text{epist}(s\datatx, a\datatx) = \entropy(\DNIG(\priorparam(\s\datatx, a\datatx), \evidence(\s\datatx, a\datatx)))$}. The entropy of the likelihood distribution represents the aleatoric uncertainty, i.e. \smash{$u_\text{alea}(s\datatx, a\datatx) = \entropy(\DNormal(\mu(\s\datatx, a\datatx), \sigma(\s\datatx, a\datatx)))$}. Indeed, it has been showed \emph{theoretically} that epistemic uncertainty increases for input states far from input states observed during training (see app.~\ref{app:proofs}) \cite{natpn}. Thus, the combination of DQN and posterior networks does not suffer from arbitrary uncertainty estimates for extreme input states contrary to ReLU networks \cite{overconfident-relu}.
