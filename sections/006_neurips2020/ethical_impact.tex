\section*{Broader Impact}
\label{ethical_impact}

Traditional classification models without uncertainty estimate can be dangerous when used without domain expertise. They might have indeed unexpected behavior in new anomalous situations/input and are unaware of the underlying risk of their predictions. Uncertainty aware models like \oursacro try to mitigate the risk of such autonomous predictions by attaching a confidence score to their predictions. On one hand, uncertainty aware predictions could be particularly beneficial in domains with potential critical consequences and prone to automation (e.g. finance, autonomous driving or medicine). When applied, these models are able to refrain from predicting if the data is out of their domain of expertise. \ours makes a significant step further in this direction by even not requiring to observe similar anomalous situations during training. Anomalous data are typically not known in advance since they are rare by definition. Thus \ours significantly increases the applicability of uncertainty estimation across application domains. On the other hand, high-quality of uncertainty estimation might also give a false sense of security. A potential risk is that an excessive trust in the model behavior leads to a lack of human supervision. For example in medicine, predictions wrongly deemed safe could have dramatic repercussions without human control.