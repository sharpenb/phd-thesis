\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{square, sort, comma, numbers}{natbib}
% before loading neurips_2021

% ready for submission
\input{include/00-packages}
%\usepackage{neurips_2021}
\usepackage{iclr2022_conference, times}
\iclrfinalcopy
\input{include/00-commands}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\everypar{\looseness=-1}
\frenchspacing

\title{Natural Posterior Network: Deep Bayesian Uncertainty for Exponential Family Distributions}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{% I'm commenting these out to avoid accidental de-anonymization (Daniel)
   Bertrand Charpentier\thanks{Equal contribution}, Oliver Borchert\footnote[1]{}, Daniel Z\"ugner, Simon Geisler, Stephan G\"unnemann\\
   Department of Informatics \& Munich Data Science Institute\\
   Technical University of Munich, Germany\\
   \texttt{\{charpent, borchero, zuegnerd, geisler, guennemann\}@in.tum.de} \\
}

\begin{document}

\maketitle

\begin{abstract}

    Uncertainty awareness is crucial to develop reliable machine learning models. In this work, we propose the \ours{} (\oursacro{}) for fast and high-quality uncertainty estimation for any task where the target distribution belongs to the exponential family. Thus, \oursacro{} finds application for both classification and general regression settings. Unlike many previous approaches, \oursacro{} does not require out-of-distribution (OOD) data at training time. Instead, it leverages Normalizing Flows to fit a single density on a learned low-dimensional and task-dependent latent space. For any input sample, \oursacro{} uses the predicted likelihood to perform a Bayesian update over the target distribution. Theoretically, \oursacro{} assigns high uncertainty far away from training data. Empirically, our extensive experiments on calibration and OOD detection show that \oursacro{} delivers highly competitive performance for classification, regression and count prediction tasks.
    
\end{abstract}

\input{include/01-introduction}
\input{include/02-related-work}
\input{include/03-model}
\input{include/04-experiments}
\input{include/05-conclusion}
\clearpage
\input{include/06-ethics-statement}
\input{include/07-reproducibility-statement}

\bibliographystyle{iclr2022_conference}
\bibliography{bibliography}

\newpage

%\input{include/xx-checklist}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
\input{include/xx-appendix}

\end{document}
