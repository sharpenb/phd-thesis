\section{Related Work}

In this section, we describe other work related to uncertainty estimation for supervised learning. We refer to \citet{uncertainty-survey} for a detailed survey on uncertainty estimation in deep learning. 

\textbf{Sampling-based methods.} A first family of models estimates uncertainty by aggregating statistics (e.g. mean and variance) from different samples of an implicit predictive distribution. Examples are ensemble \citep{bayesian-classifier-combination,ensembles, dynamic-bayesian-combination-classifiers,batch-ensembles,hyper-ensembles} and dropout \citep{dropout} models which provide high-quality uncertainty estimates \citep{dataset-shift} at the cost of an
expensive sampling phase at inference time. Moreover, ensembles usually require training multiple models. Further, Bayesian neural networks (BNN) \citep{bayesian-networks, scalable-laplace-bnn, simple-baseline-uncertainty} model the uncertainty on the weights and also require multiple samples to estimate the uncertainty on the final prediction. While recent BNNs have shown reasonably good performance \citep{rank-1-bnn,practical-bayesian,liberty-depth-bnn}, modelling the distribution on the weights suffers from pathological behavior thus limiting these approaches in practice \citep{expressiveness-bnn, practical-bnn, what-bnn-posterior}. In particular, \citet{what-bnn-posterior} uses an enormous computation budget by parallelizing the computation over 512 TPUv3  devices and running tens of thousands of training epochs to achieve a more exact Bayesian inference which is not suitable for practical applications. In contrast, \oursacro{} predicts uncertainty in \emph{a single forward pass} with a \emph{closed-form posterior distribution} over the target variable. \oursacro{} \emph{does not} model uncertainty on the weights.

\textbf{Sampling-free methods.} A second family of models is capable of estimating uncertainty in a single forward pass. The family of models parametrizing conjugate prior distributions is the main focus of this paper \citep{survey_evidential_uncertainty,evaluating_dbu,max_gap_id_ood,uncertainty-generative-classifier,multifaceted_uncertainty,graph_posterior, lightweight-prob-net}. Beyond this family of models, we differentiate between four other families of sampling-free models for uncertainty estimation. A first family aims at learning deep Gaussian processes with random features projections or learned inducing points \citep{uncertainty-distance-awareness, due, duq, uceloss}. A second family aims at learning deep energy-based models \citep{ood_ebm, jem_ebm}. Another family of models aims at propagating uncertainty across layers \citep{natural-parameter-network, sampling-free-variance-propagation, feed-forward-propagation, lightweight-prob-net, probabilistic-backprop-scalable-bnn}. They model uncertainty at the weight and/or activation levels and are generally constrained to specific transformations. In contrast, \oursacro{} only models the uncertainty on the predicted target variable and does not enforce any constraint on the encoder architecture. Further, some of the models propagating uncertainty already used the exponential family framework \citep{natural-parameter-network, deep-exponential-families}. However, while they parametrize exponential family distributions, \oursacro{} parametrizes the \emph{conjugate prior of the target exponential family distributions} which accounts for the epistemic uncertainty. Finally, while the family of calibration models aims at calibrating predictions \citep{accurate-uncertainties-deep-learning-regression, confidence-aware-learning, individual-calibration, distribution-calibration-regression, intra-order-preserving}, \oursacro{} aims at accurately modelling both aleatoric and epistemic uncertainty on in- and out-of-distribution data.
