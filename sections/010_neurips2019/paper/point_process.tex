% !TeX root = ./nips_2019.tex

\section{Point Process Framework} 

%\bc{
%\begin{itemize}
%\item Here, we want to show that with an extension using the point process framework, our models can model the distribution on classes AND time. This was not possible with the first versions of our models. Moreover it allows later to have a closer comparison with our competitors (Neural Hawkes and RMTPP)
%\item The only practical modification is the two new terms (ii) and (iii) in the loss which emerge from the maximization of the likelihood of the joint distribution instead of the conditional distribution.
%\item Conceptually the function decomposition does not learn now $\theta(\tau)$ but intensity parameters of point processes.
%\end{itemize}} \sg{for me it is not clear what we want to say here. do we want to say that our models are more flexible? that they generalize the Point Process framework??? also: are we then USING this point process idea? and what do we then actually change?? do we use eq (13) as a loss? i really don't understand this part!} \sg{or are we essentially saying that we do not model $\theta(\tau)$ as I described it; but rather the intensity function???}

Our models \DirModel and \GPModel predict $P(\theta(\DeltaTime))$, enabling to evaluate, e.g., $\overline{\bm{p}}$ after a specific time gap $\tau$. This corresponds to a conditional distribution $q(\IndexClass|\DeltaTime):=\overline{p}_{\IndexClass}(\DeltaTime)$ over the classes.
In this section, we introduce a \emph{point process} framework to generalize \DirModel to also predict the time distribution $q(\DeltaTime)$. This enables us to predict, e.g., the most likely time the next event is expected or to evaluate the joint distribution $q(c|\tau)\cdot q(\DeltaTime)$. We call the model \DirModel-PP.

We modify the model so that each class $\IndexClass$ is modelled using an inhomogeneous Poisson point process with positive locally integrable intensity function $\lambda_\IndexClass(\DeltaTime)$. Instead of generating parameters $\theta(\DeltaTime)= (\alpha_1 (\DeltaTime),...,\alpha_\NbClasses (\DeltaTime))$ by function decomposition, \DirModel-PP generates intensity parameters over time: $\smash{\log \lambda_\IndexClass(\DeltaTime) = \sum_{\IndexPoint=1}^{\NbPoints} w_\IndexPoint^{(\IndexClass)} \mathcal{N}(\DeltaTime|\DeltaTime_\IndexPoint^{(\IndexClass)}, \sigma_\IndexPoint^{(\IndexClass)}) + \nu}$. The main advantage of such general decomposition is its potential to describe complex multimodal intensity functions contrary to other models like RMTPP \cite{RMTPP} (app.~\ref{non_expressiveness_hawkes_rmtpp}). Since the concentration parameter $\alpha_\IndexClass(\DeltaTime)$ and the intensity parameter $\lambda_\IndexClass(\DeltaTime)$ both relate to the number of events of class $\IndexClass$ around time $\DeltaTime$, it is natural to convert one to the other.

Given this $\NbClasses$-multivariate point process, the probability of the next class given time and the probability of the next event time are
$\smash{q(\IndexClass|\DeltaTime) = \frac{\lambda_{\IndexClass}(\DeltaTime)}{\lambda_0(\DeltaTime)}}$ and $\smash{q(\DeltaTime) = \lambda_0(\DeltaTime) e^{-\int_{0}^{\DeltaTime} \lambda_0(s) ds}}$ where $\smash{\lambda_0(\DeltaTime) = \sum_{\IndexClass=1}^{\NbClasses} \lambda_\IndexClass(\DeltaTime)}$. Since the classes are now modelled via a point proc., the log-likelihood of the event $\Event_\IndexEvent = (\IndexClass_\IndexEvent, \DeltaTime_\IndexEvent^*)$ is:
\begin{equation}\label{eq:pp_loss}
\begin{aligned}
\log q(\IndexClass_\IndexEvent, \DeltaTime_\IndexEvent^*) = \log q(\IndexClass_\IndexEvent| \DeltaTime_\IndexEvent^*) + \log q(\DeltaTime_\IndexEvent^*) = \underbrace{\log  \frac{\lambda_{\IndexClass_\IndexEvent}(\DeltaTime_\IndexEvent^*)}{\lambda_{0}(\DeltaTime_\IndexEvent^*)}}_{(i)} + \underbrace{\log \lambda_{0}(\DeltaTime_\IndexEvent^*)}_{(ii)} - \underbrace{\int_{0}^{\DeltaTime_\IndexEvent^*}\lambda_0(t)dt}_{(iii)}
\end{aligned}
\end{equation}
The terms (ii) and (iii) act like a regularizer on the intensities by penalizing large cumulative intensity $\lambda_0(\DeltaTime)$ on the time interval $[\Timestamp_{\IndexEvent-1}, \Timestamp_\IndexEvent]$ where no events occurred. The term (i) is the standard cross-entropy loss at time $\DeltaTime_\IndexEvent$.  Or equivalently, by modeling the distribution $\textbf{Dir}(\lambda_1(\DeltaTime),..,\lambda_\NbClasses(\DeltaTime))$, we see that term (i) is equal to $\mathcal{L}_{\IndexEvent}^{\text{CE}}$ (see \cref{uncertainty_loss}). Using this insight, we obtain our final \DirModel-PP model: We achieve uncertainty on the class prediction by modeling $\lambda_\IndexClass(\DeltaTime)$ as concentration parameters of a Dirichlet distribution and train the model with the loss of \cref{eq:pp_loss} replacing term (i) by $\mathcal{L}_{\IndexEvent}^{\text{UCE}}$. As it becomes apparent \DirModel-PP differs from \DirModel only in the regularization of the loss function, enabling it to be interpreted as a point process.
