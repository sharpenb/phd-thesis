\section{Related Work}

Predictions based on discrete sequences of events regardless of time can be modelled by Markov Models \cite{MarkovModel1} or RNNs, usually with its more advanced variants like LSTMs \cite{LSTM} and GRUs \cite{GRU}. To exploit the time information some models \cite{TimeDependentRNN, PhasedLstm} additionally take time as an input but still output a single prediction for the entire future. In contrast, temporal point process framework defines the intensity function that describes the rate of events occuring over time.

\com{RMTPP \cite{RMTPP} uses an RNN to encode the event history into a vector that defines an exponential intensity function. Hence, it is able to capture complex past dependencies and model distributions resulting from simple point processes, such as Hawkes \cite{hawkes1971spectra} or self-correcting \cite{SelfCorrecting}, but not e.g.\ multimodal distributions. On the other hand, Neural Hawkes Process \cite{hawkes} uses continuous-time LSTM which allows specifying more complex intensity functions. Now the likelihood evaluation is not in closed-form anymore, but requires Monte Carlo integration. However, these approaches, unlike our models, do not provide any uncertainty in the predictions. In addition, \GPModel and \DirModel can be extended with a point process framework while having the expressive power to represent complex time evolutions.}

% To allow modelling the event together with a time of occurrence, RMTPP \cite{RMTPP} and Neural Hawkes Process \cite{hawkes} combine RNNs with a point process framework to compute the intensities at each time point. \bc{Hence, RMTPP is able to model decaying intensities (like many other point processes, e.g. Cox, Hawkes) and Neural Hawkes Process is able to model multimodal distributions. However, these approaches do not provide any uncertainty in the predictions. Should we mention the expressive power of WGP-LN and FD-DIR to model complex time evolutions ?}

%However, these approaches do not focus on approximating multimodal distributions over time neither do they provide any uncertainty in the predictions.

\com{Uncertainty in machine learning has shown a great interest \cite{BayesianRNN, PowerCertainty, Ensemble}}. For example, uncertainty can be imposed by introducing distributions over the weights \cite{WeightUncertainty, BayesianRNNForecastingUncertainty, LaplaceNN}. Simpler approaches introduce uncertainty directly on the class prediction by using Dirichlet distribution independent of time \cite{PriorNetworks, NNRBetaDir}. In contrast, the \DirModel model models complex temporal evolution of Dirichlet distribution via function decomposition which can be adapted to have a point process interpretation.

Other methods introduce uncertainty time series prediction by learning state space model with Gaussian processes \cite{StateSpaceGPIdentification, StateSpaceGP}. Alternatively, RNN architecture has been used to model the probability density function over time \cite{ProbabilityEvolutionRNN}. Compared to these models, the \GPModel model uses both Gaussian processes and RNN to model uncertainty and time. Our models are based on pseudo points. Pseudo points in a GP have been used to reduce the computational complexity \cite{SparseGP}. Our goal is not to speed up the computation, since we control the number of points that are generated, but to give them different importance. In \cite{WeightedGP} a weighted GP has been considered by rescaling points; in contrast, our model uses a custom kernel to discard (pseudo) points.
