\section{Details of experiments}

We test our models (\textbf{\GPModel}, \textbf{\DirModel} and \textbf{DPP}) against neural point process models (\textbf{RMTPP} and \textbf{Hawkes}) and simple baselines (\textbf{RNN} and \textbf{LSTM} -- getting only history as an input, \textbf{F-RNN} and \textbf{F-LSTM} -- having also the real time of the next event as an additional input; thus, they get a strong advantage!).
We test on real world (\textbf{Stack Exchange}, \textbf{Car Indicators} and \textbf{Smart Home}) and synthetic datasets (\textbf{Graph}). We show that our models consistently outperform all the other models when evaluated with class prediction accuracy and \TimeScore.

\subsection{Model selection}\label{model-selection}

We apply the same tuning technique to all models. We split all datasets into train--validation--test sets ($60\%-20\%-20\%$), use the validation set to select a model and the test set to get final scores. For Stack Exchange dataset we split on users. In all other datasets we split the trace based on time. We search over dimension of a hidden state $\{32,64,128,256\}$, batch size $\{16,32,64\}$ and $L_2$ regularization parameter $\{0, 10^{-3}, 10^{-2}, 10^{-1}\}$. We use the same learning rate $0.001$ for all models and an Adam optimizer \cite{Adam}, run each of them $5$ times for maximum of $100$ epochs with early stopping after $5$ consecutive epochs without improvement in the validation loss. For the number of points $\NbPoints$ we pick $3$ for \GPModel and $20$ for \DirModel. \GPModel and \DirModel have additional regularization (\cref{gp_regularization}) with hyperparameters $\alpha$ and $\beta$. For both models we choose $\alpha = \beta = 10^{-3}$. Model with the highest mean accuracy on the validation set is selected. We use GRU cell \cite{GRU} for both of our models. We trained all models on GPUs (1TB SSD).

\subsection{Results}\label{detail-results}

\cref{table:accuracy,table:time_error}, together with \cref{fig:all_results}
show test results for \textit{all} models on \textit{all} datasets for Class accuracy and \TimeScore.

\begin{figure}[H]
\centering
    \includegraphics[width=\linewidth]{sections/010_neurips2019/paper/images/accuracy-final-all.pdf}
    \caption{Class accuracy (top) and \TimeScore (bottom) comparison across datasets}
    \label{fig:all_results}
\end{figure}

\begin{table}
    \centering
    \caption{Class accuracy comparison for all models on all datasets}\label{table:accuracy}
    \input{sections/010_neurips2019/paper/tables/final_accuracy.tex}
\end{table}
\begin{table}
    \centering
    \caption{\TimeScore comparison for all models on all datasets}\label{table:time_error}
    \input{sections/010_neurips2019/paper/tables/final_time_error.tex}
\end{table}

\input{sections/010_neurips2019/paper/time_mse}

% \subsubsection{Car Indicators}
% RNN\\
% \input{tables/validation_table_rnn_bmw-indicator.tex}\vfill
% LSTM\\
% \input{tables/validation_table_lstm_bmw-indicator.tex}\vfill
% Future-RNN\\
% \input{tables/validation_table_future-rnn_bmw-indicator.tex}\vfill
% Future-LSTM\\
% \input{tables/validation_table_future-lstm_bmw-indicator.tex}\vfill
% RMTPP\\
% \input{tables/validation_table_rmtpp_bmw-indicator.tex}\vfill
% Hawkes\\
% \input{tables/validation_table_hawkes_bmw-indicator.tex}\vfill
% Gaussian Process\\
% \input{tables/validation_table_gaussian-process_bmw-indicator.tex}\vfill
% Dirichlet\\
% \input{tables/validation_table_dirichlet_bmw-indicator.tex}\vfill

% \subsubsection{Smart Home}
% RNN\\
% \input{tables/validation_table_rnn_kast-home.tex}\vfill
% LSTM\\
% \input{tables/validation_table_lstm_kast-home.tex}\vfill
% Future-RNN\\
% \input{tables/validation_table_future-rnn_kast-home.tex}\vfill
% Future-LSTM\\
% \input{tables/validation_table_future-lstm_kast-home.tex}\vfill
% RMTPP\\
% \input{tables/validation_table_rmtpp_kast-home.tex}\vfill
% Hawkes\\
% \input{tables/validation_table_hawkes_kast-home.tex}\vfill
% Gaussian Process\\
% \input{tables/validation_table_gaussian-process_kast-home.tex}\vfill
% Dirichlet\\
% \input{tables/validation_table_dirichlet_kast-home.tex}\vfill


% \subsubsection{K-Gaussians}
% RNN\\
% \input{tables/validation_table_rnn_shifted-gaussians.tex}\vfill
% LSTM\\
% \input{tables/validation_table_lstm_shifted-gaussians.tex}\vfill
% Future-RNN\\
% \input{tables/validation_table_future-rnn_shifted-gaussians.tex}\vfill
% Future-LSTM\\
% \input{tables/validation_table_future-lstm_shifted-gaussians.tex}\vfill
% RMTPP\\
% \input{tables/validation_table_rmtpp_shifted-gaussians.tex}\vfill
% Hawkes\\
% \input{tables/validation_table_hawkes_shifted-gaussians.tex}\vfill
% Gaussian Process\\
% \input{tables/validation_table_gaussian-process_shifted-gaussians.tex}\vfill
% Dirichlet\\
% \input{tables/validation_table_dirichlet_shifted-gaussians.tex}\vfill

% \subsubsection{Random Graph}
% RNN\\
% \input{tables/validation_table_rnn_random-graph.tex}\vfill
% LSTM\\
% \input{tables/validation_table_lstm_random-graph.tex}\vfill
% Future-RNN\\
% \input{tables/validation_table_future-rnn_random-graph.tex}\vfill
% Future-LSTM\\
% \input{tables/validation_table_future-lstm_random-graph.tex}\vfill
% RMTPP\\
% \input{tables/validation_table_rmtpp_random-graph.tex}\vfill
% Hawkes\\
% \input{tables/validation_table_hawkes_random-graph.tex}\vfill
% Gaussian Process\\
% \input{tables/validation_table_gaussian-process_random-graph.tex}\vfill
% Dirichlet\\
% \input{tables/validation_table_dirichlet_random-graph.tex}\vfill
