\begin{table}[!h]
    \centering
        \scriptsize
        \resizebox{\textwidth}{!}{
            \begin{tabular}{p{1cm} p{12cm}}
             \textbf{Node} & \textbf{Node Representation} \\
             \toprule
             \multirow{2}{*}{1637} & \textbf{Abstract:} {\tt A pervasive, yet much ignored, factor in the analysis of processing-failures is the problem of misorganized knowledge. If a systems knowledge is not indexed or organized correctly, it may make an error, not because it does not have either the general capability or specific knowledge to solve a problem, but rather because it does not have the knowledge sufficiently organized so that the appropriate knowledge structures are brought to bear on the problem at the appropriate time. In such cases, the system can be said to have forgotten the knowledge, if only in this context. This is the problem of forgetting or retrieval failure. This research presents an analysis along with a declarative representation of a number of types of forgetting errors. Such representations can extend the capability of introspective failure-driven learning systems, allowing them to reduce the likelihood of repeating such errors. Examples are presented from the Meta-AQUA program, which learns to improve its performance on a story understanding task through an introspective meta-analysis of its knowledge, its organization of its knowledge, and its reasoning processes.} \\
             & \textbf{Bag-of-Word:} {\tt['retrieval', 'number', 'problem', 'learning', 'systems', 'representations', 'knowledge', 'representation', 'understanding', 'program', 'learns', 'time', 'make', 'examples', 'appropriate', 'general', 'error', 'analysis', 'presented', 'cases', 'performance', 'correctly', 'task', 'presents', 'improve', 'reasoning', 'likelihood', 'research', 'driven', 'processes', 'does', 'reduce', 'processing', 'organization', 'extend', 'solve', 'specific', 'meta', 'factor', 'declarative', 'failure', 'capability', 'context', 'allowing', 'structures', 'types', 'ignored', 'failures', 'story', 'errors', 'sufficiently', 'brought', 'organized', 'introspective', 'bear']} \\
             
             \midrule
             
             \multirow{2}{*}{1375} & \textbf{Abstract:} {\tt This paper describes an interactive planning system that was developed inside an Intelligent Decision Support System aimed at supporting an operator when planning the initial attack to forest fires. The planning architecture rests on the integration of case-based reasoning techniques with constraint reasoning techniques exploited, mainly, for performing temporal reasoning on temporal metric information. Temporal reasoning plays a central role in supporting interactive functions that are provided to the user when performing two basic steps of the planning process: plan adaptation and resource scheduling. A first prototype was integrated with a situation assessment and a resource allocation manager subsystem and is currently being tested.} \\
             & \textbf{Bag-of-Word:} {\tt ['intelligent', 'information', 'paper', 'planning', 'based', 'case', 'integrated', 'functions', 'describes', 'decision', 'allocation', 'architecture', 'support', 'reasoning', 'mainly', 'techniques', 'process', 'supporting', 'role', 'integration', 'exploited', 'currently', 'initial', 'user', 'tested', 'interactive', 'basic', 'developed', 'temporal', 'central', 'assessment', 'situation', 'steps', 'adaptation', 'operator', 'scheduling', 'constraint', 'performing', 'plays', 'provided', 'resource', 'prototype', 'metric', 'aimed', 'plan', 'inside']} \\
             
             \midrule
             
             \multirow{2}{*}{2672}  & \textbf{Abstract:} {\tt Inductive learning systems are designed to induce hypotheses, or general descriptions of concepts, from instances of these concepts. Among the wide variety of techniques used in inductive learning systems, algorithms derived from nearest neighbour (NN) pattern classification have been receiving attention lately, mainly due to their incremental nature. Nested Generalized Exemplar (NGE) theory is an inductive learning theory which can be viewed as descent from nearest neighbour classification. In NGE theory, the induced concepts take the form of hyperrectangles in a n-dimensional Euclidean space. The axes of the space are defined by the attributes used for describing the examples. This paper proposes a fuzzified version of the original NGE algorithm, which accepts input examples given as feature/fuzzy value pairs, and generalizes them as fuzzy hyperrectangles. It presents and discusses a metric for evaluating the fuzzy distance between examples, and between example and fuzzy hyperrectangles; criteria for establishing the reliability of fuzzy examples, by strengthening the exemplar which makes the right prediction and weakening the exemplar which makes a wrong one and criteria for producing fuzzy generalizations, based on the union of fuzzy sets. Keywords : exemplar-based learning, nested generalized exemplar, nearest neighbour, fuzzy NGE.} \\
             & \textbf{Bag-of-Word:} {\tt Bag-of-Words ['paper', 'used', 'learning', 'systems', 'feature', 'theory', 'based', 'input', 'algorithm', 'generalizations', 'algorithms', 'given', 'hypotheses', 'form', 'attributes', 'examples', 'general', 'concepts', 'instances', 'evaluating', 'prediction', 'classification', 'space', 'keywords', 'presents', 'pattern', 'version', 'receiving', 'mainly', 'techniques', 'nature', 'sets', 'wide', 'right', 'makes', 'variety', 'attention', 'example', 'original', 'describing', 'distance', 'generalized', 'inductive', 'nearest', 'dimensional', 'descriptions', 'value', 'incremental', 'designed', 'viewed', 'discusses', 'derived', 'fuzzy', 'pairs', 'generalizes', 'producing', 'defined', 'descent', 'induced', 'nn', 'wrong', 'criteria', 'reliability', 'proposes', 'metric', 'induce', 'euclidean']} \\
             \bottomrule
        \end{tabular}
    }
    \vspace{5mm}
    \caption{3 abstracts and their representation from the CoraML dataset obtained after selecting the abstracts \GPNacro{} has assigned the highest feature evidence. Abstracts sorted in descending order of feature evidence.}
    \label{tab:highest_evidence_abstracts}
\end{table}


             %\midrule
             
             %\multirow{2}{*}{1368} & \textbf{Abstract:} {\tt Much recent research on modeling memory processes has focused on identifying useful indices and retrieval strategies to support particular memory tasks. Another important question concerning memory processes, however, is how retrieval criteria are learned. This paper examines the issues involved in modeling the learning of memory search strategies. It discusses the general requirements for appropriate strategy learning and presents a model of memory search strategy learning applied to the problem of retrieving relevant information for adapting cases in case-based reasoning. It discusses an implementation of that model, and, based on the lessons learned from that implementation, points towards issues and directions in refining the model.} \\
             %& \textbf{Bag-of-Word:} {\tt ['information', 'retrieval', 'problem', 'paper', 'recent', 'learning', 'relevant', 'based', 'case', 'model', 'learned', 'particular', 'appropriate', 'general', 'important', 'cases', 'support', 'presents', 'reasoning', 'implementation', 'tasks', 'modeling', 'research', 'processes', 'examines', 'applied', 'useful', 'memory', 'search', 'involved', 'discusses', 'issues', 'strategies', 'requirements', 'adapting', 'strategy', 'points', 'concerning', 'directions', 'refining', 'focused', 'question', 'criteria', 'indices', 'identifying', 'lessons', 'retrieving']]} \\
             
             %\midrule
             
             %\multirow{2}{*}{1779} & \textbf{Abstract:} {\tt We argue based upon the numbers of representations of given length, that increase in representation length is inherent in using a fixed evaluation function with a discrete but variable length representation. Two examples of this are analysed, including the use of Price's Theorem. Both examples confirm the tendency for solutions to grow in size is caused by fitness based selection.} \\
             %& \textbf{Bag-of-Word:} {\tt ['use', 'representations', 'representation', 'based', 'using', 'variable', 'given', 'function', 'examples', 'argue', 'inherent', 'solutions', 'length', 'size', 'fitness', 'evaluation', 'selection', 'discrete', 'theorem', 'including', 'increase', 'fixed', 'confirm', 'price', 'grow', 'numbers', 'caused']} \\