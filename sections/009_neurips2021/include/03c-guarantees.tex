\subsection{Uncertainty Estimation Guarantees} 
\label{sec:guarantees_009}

In this section, we provide theoretical guarantees showing that \GPNacro{} fulfills the three desiderata under mild assumptions given the specific definitions of concepts of aleatoric/epistemic uncertainty and with/without network effects presented in \cref{subsec:model_009}. Throughout this section, we consider a \GPNacro{} model parameterized with a (feature) encoder $f_{\phi}$ with piecewise ReLU activations, a PPR diffusion, and a density estimator \smash{$\prob(\z^{\text{ft}, (\nodev)} \condition \omega)$} with bounded derivatives. We present detailed proofs in appendix. 

The first theorem shows that \GPNacro{} follows des.~\ref{ax:certainty_features} and guarantees that \GPNacro{} achieves reasonable uncertainty estimation on extreme node features without network effects:
\begin{theorem}
\label{thm:axiom-feature}
Lets consider a \GPNacro{} model. Let \smash{$f_{\phi}(\x\nodeidxv)= V^{(l)}\x\nodeidxv + a^{(l)}$} be the piecewise affine representation of the ReLU network \smash{$f_{\phi}$} on the finite number of affine regions \smash{$Q^{(l)}$} \cite{understanding-nn-relu}. Suppose that \smash{$V^{(l)}$} have independent rows, then for any node $\nodev$ and almost any \smash{$\x\nodeidxv$} we have \smash{$\prob(f_{\phi}(\delta \cdot \x\nodeidxv) \condition \iclass; \vphi) \underset{\delta \rightarrow \infty}{\rightarrow} 0$}. Without network effects, it implies that \smash{$\beta_\iclass^{\text{ft}, (\nodev)} = \beta_\iclass^{\text{agg}, (\nodev)} \underset{\delta \rightarrow \infty}{\rightarrow} 0$}.
\end{theorem}
The proof relies on two main points: the equivalence of the \GPNacro{} and PostNet architectures without network effects, and the uncertainty guarantees of PostNet far from training data similarly to \cite{NatPN2021}. It intuitively states that, without network effects, \GPNacro{} predict small evidence (i.e. \smash{$\vbeta^{\text{agg}, (\nodev)} \approx \bm{0}$}) far from training features (i.e. \smash{$||\delta \cdot \x\nodeidxv|| \rightarrow \infty$}) and thus recover the prior prediction (i.e. \smash{$\valpha^{\text{post}, (\nodev)} \approx \valpha^\text{prior}$}). 
Note that contrary to \GPNacro{}, methods which do not account for node features (e.g. Label Propagation) or methods which only use ReLU activations \cite{overconfident-relu} cannot validate des.~\ref{ax:certainty_features}. Further, methods which perform aggregation steps in early layers (e.g. GCN \citep{Kipf2016}) do not separate the processing of the feature and network information making unclear if they fulfill the des.~\ref{ax:certainty_features} requirements. 

The second theorem shows that \GPNacro{} follows des.~\ref{ax:certainty_network_epistemic} and guarantees that a node $\nodev$ becomes more epistemically certain if its neighbors are more epistemically certain:
\begin{theorem}
\label{thm:axiom-network-epistemic}
Lets consider a \GPNacro{} model. Then, given a node $\nodev$, the aggregated feature evidence \smash{$\alpha_0^{\text{agg}, (\nodev)}$} is increasing if the feature evidence \smash{$\alpha_0^{\text{ft}, (\nodeu)}$} of one of its neighbors \smash{$\nodeu \in \neighbors(\nodev)$} is increasing.
\end{theorem}
The proof directly relies on \cref{eq:agg-evidence}. Intuitively, this theorem states that the epistemic uncertainty \smash{$u_\text{epist}\nodeidxv = -\alpha_0^\text{agg, (\nodev)}$} of a node $\nodev$ with network effects decreases if the epistemic uncertainty of the neighboring nodes without network effects decreases. Note that contrary to \GPNacro{}, methods which do not model the epistemic uncertainty explicitly (e.g. GCN \cite{Kipf2016}, GAT \citep{Velickovic2017} or APPNP \citep{Klicpera2018}) are not guaranteed to fulfil des.~\ref{ax:certainty_network_epistemic}. 

The third theorem shows that \GPNacro{} follows des.~\ref{ax:certainty_network_aleatoric}. It guarantees that a node $\nodev$ becomes more aleatorically uncertain if its neighbors are more aleatorically uncertain, or if a neighbor prediction disagrees more with the current node prediction:
\begin{theorem}
\label{thm:axiom-network-aleatoric}
Lets consider a \GPNacro{} model. Lets denote \smash{$\bar{\p}^\text{agg, (\nodev)} = \nicefrac{\vbeta^{\text{agg}, (\nodev)}}{\alpha_0^{\text{agg}, (\nodev)}}$} the diffused categorical prediction for node $\nodev$ where \smash{$\iclass^*$} is its winning class. Further, lets denote \smash{$\bar{\p}^\text{ft, (\nodeu)} = \nicefrac{\vbeta^{\text{ft}, (\nodev)}}{\alpha_0^{\text{ft}, (\nodev)}}$} the non-diffused categorical prediction for a node $\nodeu \in \vertices$. First, there exists normalized weights \smash{$\Pi_{v, u}^{'}$} such that \smash{$\sum_{\nodeu \in \vertices} \Pi_{v, u}^{'} \entropy{\DCat(\bar{\p}^\text{ft, (\nodeu)})} \leq \entropy{\DCat(\bar{\p}^\text{agg, (\nodev)})}$}. Second, if for any node \smash{$\nodeu \in \vertices$} the probability of $\bar{\p}_{\iclass^*}^\text{ft, (\nodeu)}$ decreases, then \smash{$\entropy{\DCat(\bar{\p}^\text{agg, (\nodev)})}$} increases.
\end{theorem}
The proof of the first part of the theorem is based on the entropy convexity. Intuitively, it states that the aleatoric uncertainty \smash{$u_\text{alea}\nodeidxv = \entropy{\DCat(\bar{\p}^\text{agg, (\nodev)})}$} of a node $\nodev$ with network effects is lower bounded by a weighted average of the aleatoric uncertainty without network effects of its soft neighborhood. The second part of the theorem intuitively states that if the prediction of a neighboring node $\nodeu$ without neighbor effects disagrees more with the current class prediction $\iclass^*$ of the node $\nodev$, then the aleatoric uncertainty \smash{$u_\text{alea}\nodeidxv = \entropy{\DCat(\bar{\p}^\text{agg, (\nodev)})}$} with network effects becomes higher. Note that contrary to \GPNacro{}, methods which do not use edges (e.g. PostNet \cite{charpentier2020}) cannot validate des.~\ref{ax:certainty_network_aleatoric} and des.~\ref{ax:certainty_network_epistemic}.