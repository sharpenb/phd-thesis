\chapter{Background}
\label{chap:background}

\section{Desiderata}

\subsection{Bayes Formula} We recall the Bayes formula since it iss a core concept in many uncertainty estimation approaches \bc{CITE}.
We define the distribution $\prob(\y \condition \bm{\theta})$ over the target variable $\y \in \real$ given the paremeter $\bm{\theta}$.
Given a dataset $\data=\{\y^{(1)}, ..., \y^{(\ndata)}\}$, the Bayes formula is:
\begin{equation}
    \prior(\bm{\theta} \condition \data) = \frac{\prob(\data \condition \bm{\theta}) \times \prior(\bm{\theta})}{\prob(\data)}
\end{equation}
where $\prob(\data \condition \bm{\theta})$ is the \emph{likelihood}, $\prior(\bm{\theta})$ is the \emph{prior}, $\prior(\bm{\theta} \condition \data)$ si the \emph{posterior}, and $\prob(\data)$ is the \emph{evidence}.
Intuitively, the Bayesian formula updates the prior belief represented by $\prior(\bm{\theta})$ into the posterior belief represented by $\prior(\bm{\theta} \condition \data)$ after oberving a dataset $\data$.
The choice of prior is crucial. According to the \emph{principle of maximum entropy} \citep{maximum-entropy-principle}, a practical choice for the prior is to enforce high entropy for the prior distribution which is usually considered less informative. \bc{However, note that the entropy is sensitive to a reparametrization of the prior distibution.}
The evidence term corresponds to a normalization constant which can often be ignored.

After observing a dataset $\data$, we can update the distribution over the target variable $\y$ in two ways.
A first option is to use a point-wise estimate of the target distribution parameter, i.e.:
\begin{equation}
    \prob(\y \condition \bm{\theta}^*)
\end{equation}
where $\bm{\theta}^*=\prior(\y \condition \bm{\theta})$ would use the maximum likelihood estimate or $\bm{\theta}^*=\prior(\bm{\theta} \condition \data)$ would use the maximum a posteriori estimate.
A second option is to integrate over all possible values for the target distribution parameter, i.e.:
\begin{equation}
    \label{eq:posterior_predictive}
    \prob(\y \condition \data) = \int \prob(\y \condition \bm{\theta}) \prior(\bm{\theta} \condition \data) d\bm{\theta}
\end{equation}
where $\prob(\y \condition \data)$ is called the posterior predictive distribution. 
This second approach is often considered to be more Bayesian since it depends on the full posterior distribution $\prior(\bm{\theta} \condition \data)$.
However, it might be costly to compute the posterior distribution since it requires integration.

In supervised learning, the goal is to predict the value of a target output variable $\y$ given an input $\x$ after obsering a dataset $\data=\{(\x^{(1)}, \y^{(1)}), ..., (\x^{(\ndata)}, \y^{(\ndata)})\}$..
In this case the posterior predictive requires to be adapted with two main options. The first option is:
\begin{equation}
    \prob(\y \condition \data, \x) = \int \prob(\y \condition \bm{\theta}, \x) \prior(\bm{\theta} \condition \data) d\bm{\theta}
\end{equation}
In this case, the parameter distribution $\prior(\bm{\theta} \condition \data)$ is not conditioned on the input $\x$. It only accounts for an uncertainty dependent on the dataset $\data$ .
The second option is:
\begin{equation}
    \prob(\y \condition \data, \x) = \int \prob(\y \condition \bm{\theta}) \prior(\bm{\theta} \condition \data, \x) d\bm{\theta}
\end{equation}
In this case, the parameter distribution $\prior(\bm{\theta} \condition \data, \x)$ is conditioned on the input $\x$. It also accounts for an uncertainty dependent on the input $\x$.

\subsection{Aleatoric, Epistemic \& Predictive Uncertainty}

The Equation~\ref{eq:posterior_predictive} involves three types of distribution (i.e. $\prob(\y \condition \bm{\theta})$, $\prior(\bm{\theta} \condition \data)$, and $\prob(\y \condition \data)$) which cover the three main sources of uncertainty (i.e. aleatoric, epistemic, and predictive).
In practice, each uncertainty type can be measured by computing the spread of its respective distribution with the (differential) entropy which represented its variability, i.e.:
\begin{align}
    & u_\text{alea} = \entropy[\prob(\y \condition \bm{\theta})]\\
    & u_\text{epist} = \entropy[\prior(\bm{\theta} \condition \data)]\\
    & u_\text{pred} = \entropy[\prob(\y \condition \data)]
\end{align}
Apart from the entropy, other uncertainty metrics commonly use the variance of the distributions to indicate their variability.

The \emph{aleatoric uncertainty} is sometimes also called data uncertainty, stochastic uncertainty or risk \bc{cite}. 
The aleatoric uncertainty is represented by the distribution $\prob(\y \condition \bm{\theta})$.
Beyond entropy computation, it can also be computed using max probability for classification \cite{malini2018}.
The aleatoric uncertainty should be high when \emph{the model does not know because of inherent noise in a given context} (e.g. noisy environment, noisy sensors, low computation resources, model mispecification) \bc{CITE computation uncertianty and Eyke}.
Given a specific context, the aleatoric uncertainty is \emph{irreducible} since additional observations cannot resolve the information loss due noisy measurements or misspecifications. 
However, aleatoric uncertainty can be reduced by using higher measurment resolution or improving the model specification. 

The \emph{epistemic uncertainty} is sometimes also called knowledge uncertainty, systematic uncertainty, knightian uncertainty  \bc{cite}.
The epistemic uncertainty is represented by the distribution $\prior(\bm{\theta} \condition \data)$.
The epistemic uncertainty should be high when \emph{the model does not know because of a lack of observed data} in the dataset $\data$.
Hence, the epistemic uncertainty is \emph{reducible} since it should decrease when collecting additional data.

The \emph{predictive uncertainty} is sometimes also called total uncertainty \bc{cite}.
The predictive uncertainty is represented by the distribution $\prob(\y \condition \data)$.
Intuitively, the predictive unceratinty aggreagates the effect of the aleatoric and epistemic uncertainty by integrating jointly the distributions $\prob(\y \condition \bm{\theta})$ and $\prior(\bm{\theta} \condition \data)$.

\subsection{Efficiency, Flexibility \& Robustness}

Uncertainty methods are also expected to have practical characteristics. 

First, an uncertainty method is expected to be \emph{efficient} at both training and testing time. 
A first aspect is \emph{time efficency} meaning that the method should be fast with low computational overhead.
As second aspect is \emph{data efficiency} meaning that the method should require as few data as possible to train.

Second, an unceratinty method is expected to be \emph{flexible}.
A first aspect is \emph{architecture flexibility} meaning that the method should easily adapt to any architectures in order to easily adatp to different input types (e.g. tabular, images, graphs, and sequential data) and output types (e.g. classification and regression).
A second aspect is \emph{optimization flexibility} meaning that the method should easily adapt to different training schemes including end-to-end training or fine-tuning based on pretrained models.

Finally, an uncertainty method is expected to be \emph{robust}.
A first aspect is \emph{natural robustness} meaning that the method should be performant even if there is some natural drift in the data. Natural drifts could be due to time evolutiion or location variability \bc{cite}.
A second aspect is \emph{adversarial robustness} meaning that the method should be performant even against adversarial pertubations which are specifically designed to fool the model. Adversarial pertrubations can be viewed as the worst-case scenario for the model. Differen method to compute adversarial pertubrations including white box attacks which do not have information about the model \bc{cite} and black box attacks which have full access to the model information \bc{cite}.

\section{Models}

\subsection{Sampling-based models}

Sampling-based models estimate uncertainty by aggregating statistics (e.g. mean and variance) from different samples from a given distribution. The prediciton generally follows a three step process:
\textbf{(1)} we sample $K$ model weights $\bm{\phi}_k$ from a distribution over the weights $\prior(\bm{\phi} \condition \data)$, i.e. $\bm{\phi}_k \sim \prior(\bm{\phi} \condition \data)$. The type of the weight distribuyion $\prior(\bm{\phi} \condition \data)$ is a key design choice. E.g. ensemble proposes to train independent model weights \cite{ensemble_simple}, MC dropout randomly drop weights with given dropout rate \cite{drop_out}, and other bayesian neural networks learn explicit distributions like Gaussians over the model weights \cite{bayesian-networks}. \textbf{(2)} We perform $K$ forward passes with the sampled model weights, i.e. $\bm{\theta}_k = f_{\bm{\phi}_k}(\x) \text{ for } k=1,..K$. This allows to implicitly sample the parameters $\bm{\theta}_k$ from the posterior distribution $\prior(\bm{\theta} \condition \data)$, i.e. $\bm{\theta}_k \sim \prior(\bm{\theta} \condition \data, \x)$. In this case, the parameters $\bm{\theta}_k$ can be the parameters of a categorical distribution for classification or the paremeters of a gaussian distribution for regression. \textbf{(3)} We aggregate the $bm{\theta}_k$ parameters to form a point estimate $bm{\theta}*$, e.g. $bm{\theta}*=\frac{1}{K}\sum_k \bm{\theta}_k$. This allows to define the target distribution $\prob(\y \condition \bm{\theta}^*)$. These methods are flexoble and allows to estimate all types of uncertainty while still being accurate. However, they often come to the cost of a higher computational cost due to the need of multiple forward passes.

\subsection{Sampling-free models}
Sampling-free models are capable of estimating uncertainty in a single forward pass. A first family of models expliticlty parametrizing the distribution $\prior(\bm{\theta} \condition \data, \x)$ \citep{survey_evidential_uncertainty,evaluating_dbu,max_gap_id_ood,uncertainty-generative-classifier,multifaceted_uncertainty,graph_posterior, lightweight-prob-net}. A second family aims at learning deep Gaussian processes on a learned latent space \citep{uncertainty-distance-awareness, due, duq, uceloss}. A third family aims at learning deep energy-based models \citep{ood_ebm, jem_ebm}. The GP and energy-based approaches might sometimes not be able to disentangle the different uncertainty types. Finally, a last family of models propagate uncertainty across layers \citep{natural-parameter-network, sampling-free-variance-propagation, feed-forward-propagation, lightweight-prob-net, probabilistic-backprop-scalable-bnn}. They model uncertainty at the weight and/or activation levels and are generally constrained to specific transformations.

\bc{Idea: make a table with references which analyzes each desideratum}

\section{Experimental setup}

In this section, we present an exhausitve summary of the main metrics used to evaluate the quality of uncertainty estimation. 
It covers calibration, correct/wrong predictions detection, OOD \& dataset shifts detection.
Beyond these tasks, uncertainty is also sometimes used in exploration/exploitation like in reinforcment learning (see Chapter~\ref{chap:reinforcement_learning}) or active learning \bc{CITE}.
We provide a collection of benchmarks which cover the different tasks in Table.~\bc{Do table}

\bc{We summarize important becnhmarks for each metric type in a Table}

\subsection{Correct \& Wrong Predictions}

It is crucial to estimate when ML models are likely to provide correct or wrong predictions. This allows to increase \emph{trust} in the model predictions, especially when the predictions are used to make important decisions. Intuitively, uncertainty estimates should be good indicators of the prediction correctness. Indeed, high uncertainty should indicate likely wrong prediction while low uncertainty should indicate likely correct predictions. Hence, uncertainty estimates are important to answer the following practical question:

\begin{center}
    \textbf{Can we detect prediction errors of ML models?}
\end{center}

In practice, each application would require to set a threshold on the uncertainty estimates. Ideally, while predictions associated with uncertainty estimates below this threshold should be correct, the predictions associated with uncertainty estimates above this threshold should be wrong. Hence, we can use evaluation metrics which compare scores (i.e. uncertainty estimates) with binary classes (i.e. correct/wrong predictions). Common metrics are based on false and true positive and negative rates given a specific threshold like precision and recall \bc{cite}. However, these metrics have the important limitation to dependend on a specific choice of threshold. Instead, there exist other evaluations like receiving operator curves (ROC) and precision-recall curves (PR) which can compare the predicion correctness and the predicted uncertainty scores for any choice of threshold. In particular, the area under the ROC curve (AUC-ROC) and the area under the PR curve (AUC-PR) \bc{CITE} are appropriate metrics to evaluate the overall performance of the uncertainty scores independently of the choice of threshold. All the data used for the evaluation of the correct and wrong predictions should be relevant to the task (i.e. every input has a corresponding output label).

\subsection{Out-Of-Distribution}

It is crucial to detect when incoming data are anomalous to increase the \emph{safety} of model predictions. The anomalous data are often considered out-of-distribution (OOD) in contrast with normal data similar to data observed during training which are considered in-distribution (ID). Intuitively, uncertainty estimates should be good indicators of anomalous data. Indeed, high uncertainty should indicate likely abnormal data while low uncertainty should indicate likely normal data. Hence, uncertainty estimates are important to answer the following practical question:

\begin{center}
    \textbf{Can we detect anomalous data?}
\end{center}

Similarly to the detection of correct and wrong predictions, the detection of anomalous data would also require to set a threshold on the uncertainty estimates. In this case, while predictions associated with uncertainty estimates below this threshold should be normal ID data, predictions associated with uncertainty estimates above this threshold should ideally be abnormal OOD data. Hence, we can also use evaluation metrics like precision, recall, AUC-ROC, and AUC-PR which compare scores (i.e. uncertainty estimates) with binary classes (i.e. ID/OOD data). While ID data should be relevant to the task (i.e. ID inputs have output labels), the OOD data should be clear anomalies (e.g. OOD data come from a different dataset) and might not be relevant to the task (e.g. OOD data are noisy inputs without output labels).

\subsection{Dataset Shifts}

It is crucial to detect and be robust against shifts in the data to increase the ease of \emph{maintenance} of ML models. Intuively, while the predictions should be robust to dataset shifts, the uncertainty estimates should increase under dataset shifts. Hence, uncertainty estimates are able to indicate when the incoming data drifts away from the training data before that the model breaks. Hence, uncertainty estimates are important to answer the following practical question:

\begin{center}
    \textbf{Can we be robust and detect data drift?}
\end{center}

In practice, the model would require to \emph{jointly} look at the evolution of the accuracy and the unceratinty estimates under different magnitudes of perturbations. Ideally, while the model should maintain high accuracy on shifted data (a.k.a. OOD generalization performance \bc{cite}), the uncertainty estimates of the model should increase on shifted data (a.k.a. OOD detection performance \bc{cite}). Further, other expectations on the predictions under dataset shifts involve maintaining good calibration \cite{dataset-shift}, high correct/wrong prediction detection performance, and high OOD detection performances (see Chapter~\ref{chap:robustness}). In this case, the shifted dataset is still relevant to the original task (i.e. inputs in the shifted dataset have output labels) but differ from the orginal ID dataset. We distinguish between natural and adversarial dataset shifts. Natural shifts correspond to natural pertubations which could occur in real world scenarios like time shifts \bc{cite}, location shifts \bc{cite}, or corrupted data \bc{cite}. In contrast, adversarial pertub shifts actions correspond to the worst-case scenario where the perturbations are designed to fool the model \bc{cite}.

\subsection{Calibration}

It is crucial to provide confidence intervals accurately reflecting the true chance of an event to happen. This allows to increase \emph{trust} and \emph{fairness} of the ML prediction even on under-represented data regions. Intuitively, if the model predict $80\%$ chance for a class to be the correct one, we would expect the model to be $80\%$ of the time correct. Hence, uncertainty estimates are important to answer the following practical question:

\begin{center}
    \textbf{Do the predicted confidence intervals correspond to the reality?}
\end{center}

In practice, the