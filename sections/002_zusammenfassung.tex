\chapter*{Zusammenfassung}
\addcontentsline{toc}{chapter}{\protect Zusammenfassung}%

Sowohl \emph{praktische} als auch \emph{theoretische} Gründe rechtfertigen, warum wir eine Unsicherheitsschätzung benötigen, um zuverlässige Modelle für maschinelles Lernen zu erstellen. Während von \emph{Unsicherheitsschätzung} erwartet wird, dass sie \emph{Vertrauen}, \emph{Sicherheit}, \emph{Fairness} bringt und \emph{Wartung} in realen Anwendungen erleichtert, ist die Unsicherheitsschätzung auch dringend erforderlich, um die reale physikalische Welt, die von Natur aus \emph{nicht-deterministisch} und \emph{teilweise beobachtbar} ist.

Darüber hinaus müssen Modelle des maschinellen Lernens auch mit verschiedenen Arten von Eingabedaten (z. B. Tabellen-, Bild-, Graph daten, sequentielle Daten) und Ausgabedaten (Klassen, reale Werte, Anzahl, Zeitereignisse) umgehen, von denen angenommen werden kann, dass sie entweder \emph{unabhängig} oder \emph{nicht unabhängig}. Während die Unabhängigkeitsannahme geeignet ist, viele Datentypen darzustellen, ist die Nicht-Unabhängigkeitsannahme besonders nützlich, um komplexe Datentypen mit Netzwerkeffekten oder Zeiteffekten darzustellen.

In dieser Dissertation betrachten wir die Unsicherheitsschätzung sowohl für unabhängige als auch für nicht unabhängige Daten. Dazu gehen wir auf drei Hauptaspekte ein. \textbf{(1)} Wir schlagen \emph{desiderata} vor, das gewünschte Verhalten der Unsicherheitsschätzung zu erfassen. Diese Desiderata umfassen sowohl aleatorische als auch erkenntnistheoretische Ungewissheit in Gegenwart von Störungen -- insbesondere kontradiktorischer Störungen -- sowie Netzwerk- oder Zeiteffekte. Außerdem analysieren wir das gewünschte Verhalten für Unsicherheitsschätzungen sowohl zur Trainings- als auch zur Testzeit. \textbf{(2)} Wir stellen eine große Familie neuer bayesscher \emph{Modelle} vor, die Unsicherheitsschätzungen zu geringen praktischen Kosten liefern. Diese Modelle zeigen eine starke empirische Leistung und haben theoretische Garantien für verschiedene Datentypen. \textbf{(3)} Wir entwickeln umfangreiche \emph{Metriken} zur Bewertung von Unsicherheitsschätzungen für praktische Aufgaben. Diese Versuchsaufbauten decken die Erkennung richtiger/falscher Vorhersagen, Out-Of-Distribution (OOD)-Erkennung, Datensatzverschiebungen und Kalibrierungsmetriken in Gegenwart von (gegensätzlichen) Störungen, Netzwerkeffekten oder Zeiteffekten ab. Schließlich analysieren wir den Nutzen der Verwendung von Unsicherheitsschätzungen, um einen guten Kompromiss zwischen Exploration und Ausbeutung mit hoher Probeneffizienz zu erreichen.