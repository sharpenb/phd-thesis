\chapter*{Zusammenfassung}
\addcontentsline{toc}{chapter}{\protect Zusammenfassung}%

Sowohl \emph{praktische} als auch \emph{theoretische} Überlegeungen zeigen auf, dass Unsicherheitsschätzung benötigen für zuverlässige Modell des maschinellen Lernens benötigt werden. Während \emph{Unsicherheitsschätzung} dazu beitragen soll \emph{Vertrauen}, \emph{Sicherheit}, und \emph{Fairness} zu schaffen, sowie die \emph{Wartung} in realen Anwendungen zu erleichtern, ist Unsicherheitsschätzung außerdem erforderlich, um die echte Welt, welche \emph{nicht-deterministisch} und nur \emph{teilweise beobachtbar} ist, adäquat abbilden zu können.

Modelle des maschinellen Lernens müssen darüber hinaus mit verschiedenen Arten von Eingabedaten -- Tabellen, Bildern, Graphen, oder sequentiellen Daten -- und Ausgabedaten -- Klassen, reale Werte, Anzahl, Zeitereignisse -- umgehen können, wobei diese sowohl als \emph{unabhängige} oder \emph{nicht unabhängige} Instanzen auftreten können. Während die Unabhängigkeitsannahme dabei hilft, verschiedene Datentypen abzubilden, ist die Nicht-Unabhängigkeitsannahme besonders im Kontext von komplexen Datentypen mit Netzwerkeffekten oder Zeiteffekten nützlich.

In dieser Dissertation betrachten wir die Unsicherheitsschätzung sowohl für unabhängige als auch für nicht unabhängige Daten und gehen hierfür auf drei Hauptaspekte ein. \textbf{(1)} Wir schlagen \emph{Desiderata} vor und diskutieren diese, um das gewünschte Verhalten der Unsicherheitsschätzung zu umreißen. Diese Desiderata umfassen sowohl aleatorische als auch epistemische Ungewissheit in Gegenwart von Störungen -- insbesondere kontradiktorischer Störungen -- sowie Netzwerk- oder Zeiteffekten. Außerdem analysieren wir das gewünschte Verhalten für Unsicherheitsschätzungen sowohl zur Trainings- als auch zur Testzeit. \textbf{(2)} Wir stellen eine Familie neuer bayesscher \emph{Modelle} vor, die Unsicherheitsschätzungen zu geringen praktischen Kosten liefern. Diese Modelle zeigen eine starke empirische Leistung und liefern theoretische Garantien für verschiedene Datentypen. \textbf{(3)} Wir entwickeln umfangreiche \emph{Metriken} zur Bewertung von Unsicherheitsschätzungen für praktische Aufgaben. Diese Versuchsaufbauten decken die Erkennung richtiger und falscher Vorhersagen, Out-Of-Distribution (OOD)-Erkennung, Datensatzverschiebungen und Kalibrierungsmetriken in Gegenwart von (gegensätzlichen) Störungen, Netzwerkeffekten oder Zeiteffekten ab. Zuletzt analysieren wir den Nutzen der Verwendung von Unsicherheitsschätzungen, um einen guten Kompromiss zwischen Exploration und Ausbeutung mit hoher Probeneffizienz zu erreichen.