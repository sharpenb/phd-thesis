



\begin{table*}[ht!]
	\centering
	%\begin{tiny}
	\resizebox{\textwidth}{!}{ %0.866
%    \begin{tabular}{lccccccc}
%    \toprule
 %   \textbf{Att. Rad.} & 0.0 &   0.1 &  0.2 &  0.5 &  1.0 &  2.0 \\
 %   \midrule
  %  & \multicolumn{6}{c}{\textbf{Smoothed models}} \\
%    \input{tables_v2/normal-cifar10-in-PGD_L2-crossentropy-confidence}
%    \midrule
%    & \multicolumn{6}{c}{\textbf{Smoothed models + adversarial training using label attacks}} \\
%    \input{tables_v2/adv-cifar10-in-PGD_L2-crossentropy-confidence}
%    \midrule
%    & \multicolumn{6}{c}{\textbf{Smoothed models + adversarial training using uncertainty attacks}} \\
%    \input{tables_v2/unc_adv-cifar10-in-PGD_L2-crossentropy-confidence}
%    \bottomrule
%    \end{tabular}
    \begin{tabular}{clccccccc}
    \toprule
    & \textbf{Att. Rad.} & 0.0 &   0.1 &  0.2 &  0.5 &  1.0 &  2.0 \\
    \midrule
    %& & \multicolumn{6}{c}{\textbf{Smoothed models}} \\
    \input{sections/008_icml2021/tables_v2/normal-CIFAR10-in-PGD_L2-crossentropy-confidence.tex}
    \midrule
    %& & \multicolumn{6}{c}{\textbf{Smoothed models + adversarial training using label attacks}} \\
    \input{sections/008_icml2021/tables_v2/adv-CIFAR10-in-PGD_L2-crossentropy-confidence}
    \midrule
    %& & \multicolumn{6}{c}{\textbf{Smoothed models + adversarial training using uncertainty attacks}} \\
    \input{sections/008_icml2021/tables_v2/unc_adv-CIFAR10-in-PGD_L2-crossentropy-confidence}
    \bottomrule
    \end{tabular}}
	%\end{tiny}
	\caption{Distinguishing between correctly and wrongly predicted labels based on differential entropy under PGD label attacks. Smoothed DBU models on CIFAR10. Column format: guaranteed lowest performance $\cdot$ empirical performance $\cdot$ guaranteed highest performance (blue: normally/adversarially trained smooth classifier is more robust than the base model).}
	\label{tab:cifar10_smooth_confidence_2}
\end{table*}

\begin{table*}[ht!]
	\centering
	%\begin{tiny}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{clcccccc}
			\toprule
			& \textbf{Att. Rad.} &   0.1 &  0.2 &  0.5 &  1.0 &  2.0 \\
			\midrule
			%& \multicolumn{6}{c}{\textbf{Smoothed models}} \\
			\input{sections/008_icml2021/tables_v2/normal-CIFAR10-in-PGD_L2-crossentropy-attack_detection}
			\midrule
			%& \multicolumn{6}{c}{\textbf{Smoothed models + adversarial training using label attacks}} \\
			\input{sections/008_icml2021/tables_v2/adv-CIFAR10-in-PGD_L2-crossentropy-attack_detection}
			\midrule
			%& \multicolumn{6}{c}{\textbf{Smoothed models + adversarial training using uncertainty attacks}} \\
			\input{sections/008_icml2021/tables_v2/unc_adv-CIFAR10-in-PGD_L2-crossentropy-attack_detection}
			\bottomrule
		\end{tabular}}
	%\end{tiny}
	\caption{Attack detection (PGD label attacks) based on differential entropy. Smoothed DBU models on CIFAR10. Column format: guaranteed lowest performance $\cdot$ empirical performance $\cdot$ guaranteed highest performance (blue: normally/adversarially trained smooth classifier is more robust than the base model).}
	\label{tab:cifar10_smooth_attackdetection_2}
\end{table*}





\begin{table*}[ht!]
	\centering
	%\begin{tiny}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{clccccccc}
			\toprule
			& \textbf{Att. Rad.} & 0.0 &   0.1 &  0.2 &  0.5 &  1.0 &  2.0 \\
			\midrule
			& & \multicolumn{6}{c}{\textbf{ID-Attack}} \\
			\input{sections/008_icml2021/tables_v2/normal-CIFAR10-in-PGD_L2-diffE-ood}
			\midrule
			%& & \multicolumn{6}{c}{\textbf{ID-Attack}} \\
			\input{sections/008_icml2021/tables_v2/adv-CIFAR10-in-PGD_L2-diffE-ood}
			\midrule
			%& & \multicolumn{6}{c}{\textbf{ID-Attack}} \\
			\input{sections/008_icml2021/tables_v2/unc_adv-CIFAR10-in-PGD_L2-diffE-ood}
			\midrule
			\midrule
			& & \multicolumn{6}{c}{\textbf{OOD-Attack}} \\
			\input{sections/008_icml2021/tables_v2/normal-CIFAR10-out-PGD_L2-diffE-ood}
			\midrule
			%& & \multicolumn{6}{c}{\textbf{OOD-Attack}} \\
			\input{sections/008_icml2021/tables_v2/adv-CIFAR10-out-PGD_L2-diffE-ood}
			\midrule
			%& & \multicolumn{6}{c}{\textbf{OOD-Attack}} \\
			\input{sections/008_icml2021/tables_v2/unc_adv-CIFAR10-out-PGD_L2-diffE-ood}
			\bottomrule
		\end{tabular}}
	%\end{tiny}
	\caption{OOD detection based on differential entropy under PGD uncertainty attacks against differential entropy on ID data and OOD data. Smoothed DBU models on CIFAR10. Column format: guaranteed lowest performance $\cdot$ empirical performance $\cdot$ guaranteed highest performance (blue: normally/adversarially trained smooth classifier is more robust than the base model).}
	\label{tab:cifar10_smooth_ooddetection_2}
%\vspace*{0.5cm} % hack so that we do not have text at the bottom of page!!
\end{table*}











\subsection{How to make DBU models more robust?}


Our robustness analysis based on label attacks and uncertainty attacks shows that predictions, uncertainty estimation and the differentiation between ID and OOD data are not robust. Next, we explore approaches to improve robustness properties of DBU models w.r.t.\ these tasks based on randomized smoothing and adversarial training. 

\paragraph{Randomized smoothing} was originally proposed for certification of classifiers \cite{cohen2019}.
The core idea is to draw multiple samples $\x\dataix_s \sim \DNormal(\x\dataix, \sigma)$ around the input data $\x\dataix$, to feed all these samples through the neural network, and to aggregate the resulting set of predictions (e.g. by taking their mean), to get a smoothed prediction. Besides allowing certification, as a side effect, the smoothed model is more robust. Our idea is to use randomized smoothing to improve robustness of DBU models, particularly w.r.t.\ uncertainty estimation. In contrast to discrete class predictions, however, certifying uncertainty estimates such as differential entropy scores requires a smoothing approach that is able to handle continuous values as in regression tasks. So far, only few works for randomized smoothing for regression models have been proposed \citep{confidence_certificate_rs,median_smoothing}. We choose median smoothing \citep{median_smoothing}, because it is applicable to unbounded domains as required for the uncertainty estimates covered in this work. In simple words: The set of uncertainty scores obtained from the $\x\dataix_s \sim \DNormal(\x\dataix, \sigma)$ is aggregated by taking their median. 

In the following experiments we focus on differential entropy as the uncertainty score. We denote the resulting smoothed differential entropy, i.e. the median output, as $m(\x\dataix)$.
Intuitively, we expect that the random sampling around a data point as well as the outlier-insensitivity of the median to improve the robustness of the uncertainty estimates w.r.t.\ adversarial examples.

To measure the performance and robustness of our smoothed DBU models, we apply median smoothing on the same tasks as in the previous sections, i.e., distinguishing between correctly and wrongly labeled inputs, attack detection, OOD detection and compute the corresponding AUC-PR score under label attacks and uncertainty attacks. 
The bold, middle part of the columns in Tables~\ref{tab:cifar10_smooth_confidence_2}, \ref{tab:cifar10_smooth_attackdetection_2}, and~\ref{tab:cifar10_smooth_ooddetection_2} show the AUC-PR scores on CIFAR10, which we call \emph{empirical performance} of the smoothed models. To facilitate the comparison with the base model of Section~\ref{sec:attack_dirichlet_model_008}, we highlight the AUC-PR scores in blue in cases where the smooth model is more robust. The highlighting clearly shows that randomized smoothing increases the robustness of the empirical performance on OOD detection. 
OOD detection under strong PGD attacks (attack radius $\geq 0.5$) performs comparable to random guessing (i.e. AUC-PR scores around $50\%$ whith $50\%$ ID and $50\%$ OOD data). This shows that DBU models are not reliably efficient w.r.t. this task.
%Under strong attack (attack radius $\geq 50\%$ OOD detection performance decreases until it becomes similar to random guessing, which results in an  AUC-PR scores of $50\%$ (for 50~\% ID and 50~\% OOD data). 
In attack detection and distinguishing between correctly and wrongly predicted labels the smoothed DBU model are mostly more robust than the base models for attack radii $\geq 0.5$.

\paragraph{Certified performance.} Using the median based on smoothing improves the empirical robustness, but it does not provide formal guarantees how low/high the performance might actually get under perturbed data (since any attack is only a heuristic). 
Here, we propose novel guarantees by exploiting the individual certificates we obtain via randomized smoothing.
 Note that the certification procedure \citep{median_smoothing} enables us to derive lower and upper bounds $\underline{m}(\x\dataix) \leq m(\x\dataix) \leq \overline{m}(\x\dataix)$ which hold with high probability and indicate how much the median might change in the worst-case when $\x\dataix$ gets perturbed subject to a specific (attack) radius.
 
These bounds allow us to compute certificates that bound the performance of the smooth models, which we refer to as the \emph{guaranteed lowest performance} and \emph{guaranteed highest performance}. More precisely, for the guaranteed lowest performance of the model we take the pessimistic view that all ID data points realize their individual upper bounds $\overline{m}(\x\dataix)$, i.e.\ have their highest possible uncertainty (worst case). On the other hand, we assume all OOD samples realize their lower bounds $\underline{m}(\x\dataix_s)$. Using these values as the uncertainty scores for all data points we obtain the guaranteed lowest performance of the model. 
A guaranteed lowest performance of e.g. $35.0$ means that even under the worst case conditions an attack is not able to decrease the performance below $35.0$. 
Analogously, we can take the optimistic view to obtain the guaranteed highest performance of the smoothed models. 
%
Tables~\ref{tab:cifar10_smooth_confidence_2}, \ref{tab:cifar10_smooth_attackdetection_2} and~\ref{tab:cifar10_smooth_ooddetection_2} show the guaranteed lowest/highest performance (non-bold, left/right of the empirical performance). 
%\sg{can we add a bit more discussion. about some specific cases for example. also again explaining: e.g. 33.1 shows that even under the worst assumption an attack can only drop the performance to 33.1}
Our results show that the difference between guaranteed highest and guaranteed lowest performance increases with the attack radius, which might be explained by the underlying lower/upper bounds on the median being tighter for smaller perturbations. 
%


\paragraph{Adversarial training.}
Randomized smoothing improves robustness of DBU models and allows us to compute performance guarantees. However, an open question is whether it is possible to increase robustness even further by combining it with adversarial training. To obtain adversarially trained models we augment the data set using perturbed samples that are computed by PGD attacks against the cross-entropy loss (label attacks) or the differential entropy (uncertainty attacks). These perturbed samples $\tilde{\x}\dataix$ are computed during each epoch of the training based on inputs $\x\dataix$ and added to the training data (with the label $y\dataix$ of the original input). 
Tables~\ref{tab:cifar10_smooth_confidence_2}, \ref{tab:cifar10_smooth_attackdetection_2}, and~\ref{tab:cifar10_smooth_ooddetection_2} illustrate the results. We choose the attack radius used during training and the $\sigma$ used for smoothing to be equal. %(i.e. the entry in row \PostNet, adversarially trained using label attacks at Att. Rad. 0.1 corresponds to a \PostNet model trained using label attacks with radius 0.1 and certified with radius 0.1). 
To facilitate comparison, we highlight the empirical performance of the adversarially trained models in blue if it is better than the performance of the base model. Our results show that the additional use of adversarial training has a minor effect on the robustness and does not result in a significant further increase of the robustness. 

% Final sentences
We conclude that median smoothing is a promising technique to increase robustness w.r.t.\ distinguishing between correctly labeled samples and wrongly labeled samples, attack detection and differentiation between in-distribution data and out-of-distribution data of all Dirichlet-based uncertainty models, while additional adversarial training has a minor positive effect on robustness. 


