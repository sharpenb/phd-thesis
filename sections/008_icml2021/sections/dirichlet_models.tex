\section{Dirichlet-based uncertainty models}
\label{sec:dirichlet_models_008}
%
Standard (softmax) neural networks predict the parameters of a categorical distribution \smash{$\vp\dataix = [p\dataix_1, \ldots, p\dataix_\nclass]$} for a given input \smash{$\x\dataix \in \mathbb{R}^{d}$}, where $\nclass$ is the number of classes. 
Given the parameters of a categorical distribution, the \emph{ aleatoric uncertainty} can be evaluated. The aleatoric uncertainty is the uncertainty on the class label prediction \smash{$y\dataix \in \{1, \ldots, \nclass\}$}. For example if we predict the outcome of an unbiased coin flip, the model is expected to have high aleatoric uncertainty and predict $p(\text{head})=0.5$.

\begin{table*}[ht]
	\centering
	\resizebox{.9 \textwidth}{!}{%
		\begin{tabular}{lllllll}
			\toprule
			{} &  \textbf{$\alpha\dataix$-parametrization} & \textbf{Loss} & \textbf{OOD training data} & \textbf{Ensemble training} & \textbf{Density estimation}\\
			\midrule
			\textbf{\PostNet} & $f_{\theta}(\x\dataix) = \mathbf{1} + \bm{\alpha}\dataix$ & Bayesian loss & No & No & Yes \\
			\textbf{\PriorNet} & $f_{\theta}(\x\dataix) = \bm{\alpha}\dataix$ & Reverse KL & Yes &  No & No \\
			\textbf{\DDNet} & $f_{\theta}(\x\dataix) = \bm{\alpha}\dataix$ & Dir. Likelihood & No & Yes & No \\
			\textbf{\EvNet} & $f_{\theta}(\x\dataix) =  \mathbf{1} + \bm{\alpha}\dataix$ & Expected MSE & No &  No & No \\
			\bottomrule
		\end{tabular}
	}
	%\vspace{-.5cm}
	\caption{Summary of DBU models. Further details on the loss functions are provided in the appendix.}
	\label{tab:dirichlet_models}
\end{table*}


In contrast to standard (softmax) neural networks, DBU models predict the parameters of a Dirichlet distribution -- the natural prior of categorical distributions -- given input~$\x \dataix$ (i.e. \smash{$q\dataix = \text{Dir}(\bm{\alpha}\dataix)$} where \smash{$f_{\theta}(\x\dataix) = \bm{\alpha} \dataix \in \mathbb{R}_+^\nclass$}). Hence, the \emph{epistemic distribution} \smash{$q\dataix$} expresses the \emph{epistemic} uncertainty on $\x \dataix$, i.e. the uncertainty on the categorical distribution prediction \smash{$\vp\dataix$}. From the epistemic distribution, follows an estimate of the \emph{aleatoric distribution} of the class label prediction $\text{Cat}(\bar{\vp}\dataix)$ where \smash{$\E_{q\dataix}[\vp\dataix] = \bar{\vp}\dataix$}.
An advantage of DBU models is that one pass through the neural network is sufficient to compute epistemic distribution, aleatoric distribution, and predict the class label:
%
\begin{equation}
\begin{aligned}
    q^{(\idata)}           = \text{Dir}(\bm{\alpha}\dataix), \hspace{5pt}
    \bar{p}_\iclass\dataix  = \frac{\alpha_\iclass\dataix}{\alpha_0\dataix}, \hspace{5pt}
    y^{(\idata)}           = \arg \max_{\iclass} \;[\bar{p}_\iclass\dataix]
\end{aligned}
\end{equation}
%
where $\alpha_0\dataix = \sum^{\nclass}_{\iclass=1} \alpha_\iclass\dataix$. This parametrization allows to compute classic uncertainty measures in closed-form such as the total pseudo-count $m\dataix_{\alpha_0} = \sum_\iclass \alpha\dataix_\iclass$, the differential entropy of the Dirichlet distribution $m\dataix_\text{diffE} = h(\text{Dir}(\bm{\alpha}\dataix))$ or the mutual information $m\dataix_\text{MI} = I(y\dataix, \vp\dataix)$ (App. \ref{subsec:appendix_measurecomp}, \citep{malini2018}). 
Hence, these measure can efficiently be used to assign high uncertainty to unknown data, which makes DBU models specifically suited for detection of OOD samples. 





Several recently proposed models for uncertainty estimations belong to the family of DBU models, such as \PriorNet, \EvNet, \DDNet and \PostNet. These models differ in terms of their parametrization of the Dirichlet distribution, the training, and density estimation. An overview of theses differences is provided in Table \ref{tab:dirichlet_models}. In our study we evaluate all recent versions of these models.


Contrary to the other models, Prior Networks \textbf{(\PriorNet)} \citep{malini2018, malinin2019}  require OOD data for training to ``teach'' the neural network the difference between ID and OOD data. \PriorNet is trained with a loss function consisting of two KL-divergence terms. The fist term is designed to learn Dirichlet parameters for ID data, while the second one is used to learn a flat Dirichlet distribution %($\boldsymbol{\alpha} = \boldsymbol{1}$) 
for OOD data: 
%
\begin{equation}
\begin{aligned}
    L_{\mathrm{\PriorNet}} &= \frac{1}{N} \left[\sum_{\x\dataix \in \text{ID data}}  [\mathrm{KL} [\mathrm{Dir} (\alpha^{\mathrm{ID}}) || q\dataix]]  \right. \\
                           &+ \left.\sum_{\x\dataix \in OOD data} [\mathrm{KL} [\mathrm{Dir} (\alpha^{\mathrm{OOD}}) || q\dataix]]\right] \\
\end{aligned}
\end{equation}
%
where $\alpha^{\mathrm{ID}}$ and $\alpha^{\mathrm{OOD}}$ are hyper-parameters. Usually $\alpha^{\mathrm{ID}}$ is set to $1e^{1}$ for the correct class and $1$ for all other classes, while $\alpha^{\mathrm{OOD}}$ is set to $\mathbf{1}$ for all classes.
%
There a two variants of \PriorNet. The first one is trained based on reverse KL-divergence \citep{malinin2019}, while the second one is trained with KL-divergence \citep{malini2018}. In our experiments, we include the most recent reverse version of \PriorNet, as it shows superior performance \citep{malinin2019}. 

Evidential Networks \textbf{(\EvNet)} \citep{sensoy2018} are trained with a loss that computes the sum of squares between the on-hot encoded true label $\vy*\dataix$ and the predicted categorical $\vp\dataix$ under the Dirichlet distribution:
%
\begin{equation}
\begin{aligned}
    L_{\mathrm{\EvNet}} &= \frac{1}{N} \sum_i \E_{\vp\dataix \sim \text{Dir}(\bm{\alpha}\dataix)}||\vy*\dataix - \vp\dataix||^2 \\
\end{aligned}
\end{equation}

Ensemble Distribution Distillation \textbf{(\DDNet)} \citep{malinin2019ensemble} is trained in two steps. First, an ensemble of $M$ classic neural networks needs to be trained. 
Then, the soft-labels \smash{$\{\vp_{m}\dataix\}_{m=1}^{M}$} provided by the ensemble of networks are distilled into a Dirichlet-based network by fitting them with the maximum likelihood under the Dirichlet distribution: 
\begin{equation}
\begin{aligned}
    L_{\mathrm{\DDNet}} &= - \frac{1}{N}  \sum_i \sum_{m=1}^{M} [\ln q\dataix(\pi^{im})] \\
\end{aligned}
\end{equation}
where $\pi^{im}$ denotes the soft-label of $m$th neural network. 

Posterior Network \textbf{(\PostNetacro{})} \citep{charpentier2020} performs density estimation for ID data with normalizing flows and uses a Bayesian loss formulation: %composed of the Uncertain Cross-Entropy loss \citep{uncertainty_time} and an entropy regularizer inspired by Bayesian theory.
\begin{equation}
\begin{aligned}
    L_{\mathrm{\PostNet}} &= \frac{1}{N} \sum_i \E_{q(p\dataix)}  [\mathrm{CE} (p\dataix, y\dataix)] - H(q\dataix)
\end{aligned}
\end{equation}
where $\mathrm{CE}$ denotes the cross-entropy.
%
All loss functions can be computed in closed-form. For more details please have a look at the original paper on \PriorNet \citep{malini2018}, \PostNet \citep{charpentier2020}, \DDNet \citep{malinin2019} and \EvNet \citep{sensoy2018}.
Note that \EvNet and \PostNet model the Dirichlet parameters as \smash{$f_{\theta}(\x\dataix) = 1 + \bm{\alpha}\dataix$} while \PriorNet, \RevPriorNet and \DDNet compute them as \smash{$f_{\theta}(\x\dataix) = \bm{\alpha}\dataix$}. 








